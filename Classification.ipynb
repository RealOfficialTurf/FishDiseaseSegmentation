{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79330542-c9bd-42d4-afe7-16cc5b8bd93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "import copy\n",
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    "DEVICE = torch.device(dev)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5de9b-a7b5-41b7-b9d6-ad774e9fd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgdir, size=None):\n",
    "        \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "          Args:\n",
    "            data: Data containing video file paths.\n",
    "        \"\"\"\n",
    "        self.imgdir = imgdir\n",
    "        import os\n",
    "        self.imagefns = []\n",
    "        labeldict = {v: k for k, v in enumerate(os.listdir(self.imgdir))}\n",
    "        self.labels = []\n",
    "        for i in os.listdir(self.imgdir):\n",
    "            for j in os.listdir(self.imgdir+'\\\\'+i):\n",
    "                self.imagefns.append(i+'\\\\'+j)\n",
    "                self.labels.append(labeldict[i])\n",
    "        assert(len(self.imagefns)==len(self.labels))\n",
    "        self.resizer = None\n",
    "        if size != None:\n",
    "            self.resizer = torchvision.transforms.Resize(size,torchvision.transforms.InterpolationMode.BICUBIC,antialias=False)\n",
    "    def __len__(self):\n",
    "        return len(self.imagefns)\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.imgdir+'\\\\'+self.imagefns[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = torchvision.io.read_image(filename,torchvision.io.ImageReadMode.RGB)\n",
    "        if self.resizer != None:\n",
    "            image = self.resizer(image)\n",
    "        return image, label\n",
    "def display(display, label, save=None):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    title = label\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.title(label)\n",
    "    if display.dim() == 3:\n",
    "        plt.imshow(display.permute(1,2,0)) #pytorch loves channels first but matplotlib loves channels last\n",
    "    else:\n",
    "        plt.imshow(display)\n",
    "    plt.axis('off')\n",
    "    if save!=None:\n",
    "        plt.savefig(save) #save before show\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6974744-bb9c-4e34-91b2-f2acc779e459",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72738947-401a-4a67-8868-ed570d897d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(num,channels,filters,addpool=False,reversed=False,dropout=0):\n",
    "    block = torch.nn.Sequential()\n",
    "    if addpool:\n",
    "        block.append(torch.nn.MaxPool2d(2, stride=2))\n",
    "    if reversed:\n",
    "        for i in range(num-1):\n",
    "            block.append(torch.nn.Conv2d(channels,channels,kernel_size=(3,3),padding='same',bias=False)) #Bias is pointless for BatchNorm\n",
    "            block.append(torch.nn.BatchNorm2d(channels))\n",
    "            block.append(torch.nn.ReLU())\n",
    "        block.append(torch.nn.Conv2d(channels,filters,kernel_size=(3,3),padding='same',bias=False)) #Bias is pointless for BatchNorm\n",
    "        block.append(torch.nn.BatchNorm2d(filters))\n",
    "        block.append(torch.nn.ReLU())\n",
    "    else:\n",
    "        for i in range(num):\n",
    "            if channels == None:\n",
    "                block.append(torch.nn.LazyConv2d(filters,kernel_size=(3,3),padding='same',bias=False)) #Bias is pointless for BatchNorm\n",
    "            else:\n",
    "                block.append(torch.nn.Conv2d(channels,filters,kernel_size=(3,3),padding='same',bias=False)) #Bias is pointless for BatchNorm\n",
    "            block.append(torch.nn.BatchNorm2d(filters))\n",
    "            block.append(torch.nn.ReLU())\n",
    "            channels = filters\n",
    "    if dropout > 0:\n",
    "        block.append(torch.nn.Dropout2d(dropout))\n",
    "    return block\n",
    "class BottleResBlock(torch.nn.Module):\n",
    "    def __init__(self,channels,bottlechan,filters,stride=1,dilation=1):\n",
    "        super().__init__()\n",
    "        self.bottleneck = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(channels,bottlechan,kernel_size=(1,1),padding='valid',bias=False), #Bias is pointless for BatchNorm\n",
    "            torch.nn.BatchNorm2d(bottlechan),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(bottlechan,bottlechan,kernel_size=(3,3),padding=1+dilation-1,stride=stride,dilation=dilation,bias=False),\n",
    "            torch.nn.BatchNorm2d(bottlechan),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(bottlechan,filters,kernel_size=(1,1),padding='valid',bias=False),\n",
    "            torch.nn.BatchNorm2d(filters)\n",
    "        )\n",
    "        self.skip = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(channels,filters,kernel_size=(1,1),padding='valid',stride=stride,bias=False),\n",
    "            torch.nn.BatchNorm2d(filters),\n",
    "        ) if channels != filters or stride != 1 else torch.nn.Identity()\n",
    "    def forward(self,x):\n",
    "        x_1 = self.skip(x)\n",
    "        x_2 = self.bottleneck(x)\n",
    "        y = torch.add(x_1,x_2)\n",
    "        return y\n",
    "class InvertedBottleResBlock(torch.nn.Module):\n",
    "    def __init__(self,channels,expansion,filters,stride=1):\n",
    "        super().__init__()\n",
    "        self.bottleneck = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(channels,channels*expansion,kernel_size=(1,1),padding='valid',bias=False), #Bias is pointless for BatchNorm\n",
    "            torch.nn.BatchNorm2d(channels*expansion),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(channels*expansion,channels*expansion,kernel_size=(3,3),padding=1,stride=stride,groups=channels*expansion,bias=False), #DWConv\n",
    "            torch.nn.BatchNorm2d(channels*expansion),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(channels*expansion,filters,kernel_size=(1,1),padding='valid',bias=False),\n",
    "            torch.nn.BatchNorm2d(filters) #No ReLU here\n",
    "        )\n",
    "        self.skip = stride == 1 and channels == filters\n",
    "    def forward(self,x):\n",
    "        if self.skip:\n",
    "            x_1 = self.bottleneck(x)\n",
    "            y = torch.add(x_1,x)\n",
    "            return y\n",
    "        else:\n",
    "            y = self.bottleneck(x)\n",
    "            return y\n",
    "class PPM(torch.nn.Module):\n",
    "    def __init__(self,in_channels,pools=[1,2,4,8]):\n",
    "        super().__init__()\n",
    "        def poollayer(size):\n",
    "            block = torch.nn.Sequential()\n",
    "            if size != 1:\n",
    "                block.append(torch.nn.AvgPool2d(size, stride=size))\n",
    "            block.append(torch.nn.Conv2d(in_channels,in_channels//4,kernel_size=(1,1),bias=False))\n",
    "            block.append(torch.nn.BatchNorm2d(in_channels//4))\n",
    "            block.append(torch.nn.ReLU())\n",
    "            if size != 1:\n",
    "                block.append(torch.nn.Upsample(scale_factor=size, mode='bilinear'))\n",
    "            return block\n",
    "        self.pools = torch.nn.ModuleList([poollayer(x) for x in pools])\n",
    "        \n",
    "        #self.pool1 = poollayer(pools[0])\n",
    "        #self.pool2 = poollayer(pools[1])\n",
    "        #self.pool3 = poollayer(pools[2])\n",
    "        #self.pool4 = poollayer(pools[3])\n",
    "    def forward(self, x):\n",
    "        x_pooled = [pool(x) for pool in self.pools]\n",
    "        x_pooled.append(x)\n",
    "        y = torch.cat(x_pooled,1)\n",
    "        #x1 = self.pool1(x)\n",
    "        #x2 = self.pool2(x)\n",
    "        #x3 = self.pool3(x)\n",
    "        #x4 = self.pool4(x)\n",
    "        \n",
    "        #y = torch.cat((x,x1,x2,x3,x4),1)\n",
    "        return y\n",
    "class ASPP(torch.nn.Module):\n",
    "    def __init__(self,in_channels,out_channel_per_branch,dilations=[6,12,18]):\n",
    "        super().__init__()\n",
    "        def dilatedlayer(rate):\n",
    "            block = torch.nn.Sequential()\n",
    "            #PADDING HAS TO BE LIKE THIS\n",
    "            block.append(torch.nn.Conv2d(in_channels,out_channel_per_branch,kernel_size=(3,3),padding=rate,dilation=rate,bias=False))\n",
    "            block.append(torch.nn.BatchNorm2d(out_channel_per_branch))\n",
    "            block.append(torch.nn.ReLU())\n",
    "            return block\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels,out_channel_per_branch,kernel_size=(1,1),bias=False),\n",
    "            torch.nn.BatchNorm2d(out_channel_per_branch),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.dilations = torch.nn.ModuleList([dilatedlayer(x) for x in dilations])\n",
    "        self.globalpool = torch.nn.Sequential(\n",
    "            torch.nn.AdaptiveAvgPool2d(1),\n",
    "            torch.nn.Conv2d(in_channels,out_channel_per_branch,kernel_size=(1,1),bias=False),\n",
    "            torch.nn.BatchNorm2d(out_channel_per_branch),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.finalconv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(out_channel_per_branch*(2+len(self.dilations)),out_channel_per_branch,kernel_size=(1,1),bias=False),\n",
    "            torch.nn.BatchNorm2d(out_channel_per_branch),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        input_spatial_dim = x.size()[2:]\n",
    "        x_pooled = [pool(x) for pool in self.dilations]\n",
    "        x_pooled.append(self.conv1(x))\n",
    "        x_pooled.append(torch.nn.functional.interpolate(self.globalpool(x), input_spatial_dim, mode='bilinear', align_corners=True))\n",
    "        y = self.finalconv(torch.cat(x_pooled,1))\n",
    "        return y\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self,in_channels,classes,layers=[3,4,6,3]):\n",
    "        super().__init__()\n",
    "        def ResNetBlock(num,channels,bottlechan,filters,stride=1,addpool=False):\n",
    "            blocks = torch.nn.Sequential()\n",
    "            if addpool:\n",
    "                blocks.append(torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "            for i in range(num):\n",
    "                if stride != 1 and i == 0:\n",
    "                    blocks.append(BottleResBlock(channels,bottlechan,filters,stride))\n",
    "                else:\n",
    "                    blocks.append(BottleResBlock(channels,bottlechan,filters))\n",
    "                channels = filters\n",
    "            return blocks\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels,64,kernel_size=(7,7),stride=2,padding=3,bias=False),\n",
    "                torch.nn.BatchNorm2d(64),\n",
    "                torch.nn.ReLU(),\n",
    "            ),\n",
    "            ResNetBlock(layers[0],64,64,256,addpool=True),\n",
    "            ResNetBlock(layers[1],256,128,512,stride=2),\n",
    "            ResNetBlock(layers[2],512,256,1024,stride=2),\n",
    "            ResNetBlock(layers[3],1024,512,2048,stride=2),\n",
    "        ])\n",
    "        self.pooling = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = torch.nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.classifier = torch.nn.Linear(2048,classes)\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks)):\n",
    "            x = self.blocks[i](x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.flatten(x)\n",
    "        y = self.classifier(x)\n",
    "        return y\n",
    "class VGG16(torch.nn.Module):\n",
    "    def __init__(self,in_channels,classes):\n",
    "        super().__init__()\n",
    "        channels = [in_channels,64,128,256,512,512]\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "            ConvBlock(2,channels[0],channels[1]),\n",
    "            ConvBlock(2,channels[1],channels[2]),\n",
    "            ConvBlock(3,channels[2],channels[3]),\n",
    "            ConvBlock(3,channels[3],channels[4]),\n",
    "            ConvBlock(3,channels[4],channels[5])\n",
    "        ])\n",
    "        self.pools = torch.nn.ModuleList([\n",
    "            torch.nn.MaxPool2d(2, stride=2),\n",
    "            torch.nn.MaxPool2d(2, stride=2),\n",
    "            torch.nn.MaxPool2d(2, stride=2),\n",
    "            torch.nn.MaxPool2d(2, stride=2),\n",
    "            torch.nn.MaxPool2d(2, stride=2)\n",
    "        ])\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((7,7))\n",
    "        self.flatten = torch.nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.fcs = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(7*7*channels[5],4096,bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            torch.nn.Linear(4096,4096,bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=0.5),\n",
    "        ])\n",
    "        self.classifier = torch.nn.Linear(4096,classes)\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks)):\n",
    "            x = self.blocks[i](x)\n",
    "            x = self.pools[i](x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        for i in range(len(self.fcs)):\n",
    "            x = self.fcs[i](x)\n",
    "        y = self.classifier(x)\n",
    "        return y\n",
    "class VGG13(VGG16):\n",
    "    def __init__(self,in_channels,classes):\n",
    "        super().__init__(in_channels,classes)\n",
    "        channels = [in_channels,64,128,256,512,512]\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "            ConvBlock(2,channels[0],channels[1]),\n",
    "            ConvBlock(2,channels[1],channels[2]),\n",
    "            ConvBlock(2,channels[2],channels[3]),\n",
    "            ConvBlock(2,channels[3],channels[4]),\n",
    "            ConvBlock(2,channels[4],channels[5])\n",
    "        ])\n",
    "class VGG19(VGG16):\n",
    "    def __init__(self,in_channels,classes):\n",
    "        super().__init__(in_channels,classes)\n",
    "        channels = [in_channels,64,128,256,512,512]\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "            ConvBlock(2,channels[0],channels[1]),\n",
    "            ConvBlock(2,channels[1],channels[2]),\n",
    "            ConvBlock(4,channels[2],channels[3]),\n",
    "            ConvBlock(4,channels[3],channels[4]),\n",
    "            ConvBlock(4,channels[4],channels[5])\n",
    "        ])\n",
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self,in_channels,classes,convblocks=None,encoderblocks=[2,2,2,2,2],decoderblocks=[2,2,2,2],upsample_output=1):\n",
    "        super().__init__()\n",
    "        self.channels = [64,128,256,512,1024]\n",
    "        if type(convblocks) is torch.nn.ModuleList:\n",
    "            self.encoderblocks = torch.nn.ModuleList([\n",
    "                convblocks[0],\n",
    "                convblocks[1],\n",
    "                convblocks[2],\n",
    "                convblocks[3],\n",
    "                convblocks[4]\n",
    "            ])\n",
    "            self.upsamplers = torch.nn.ModuleList([\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.LazyConvTranspose2d(self.channels[3],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                ),\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(self.channels[3],self.channels[2],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                ),\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(self.channels[2],self.channels[1],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                ),\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(self.channels[1],self.channels[0],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                )\n",
    "            ])\n",
    "            self.decoderblocks = torch.nn.ModuleList([\n",
    "                ConvBlock(decoderblocks[0],self.channels[4],self.channels[3]),\n",
    "                ConvBlock(decoderblocks[1],self.channels[3],self.channels[2]),\n",
    "                ConvBlock(decoderblocks[2],self.channels[2],self.channels[1]),\n",
    "                ConvBlock(decoderblocks[3],self.channels[1],self.channels[0])\n",
    "            ])\n",
    "        else:\n",
    "            self.encoderblocks = torch.nn.ModuleList([\n",
    "                ConvBlock(encoderblocks[0],in_channels,self.channels[0]),\n",
    "                ConvBlock(encoderblocks[1],self.channels[0],self.channels[1],True),\n",
    "                ConvBlock(encoderblocks[2],self.channels[1],self.channels[2],True),\n",
    "                ConvBlock(encoderblocks[3],self.channels[2],self.channels[3],True),\n",
    "                ConvBlock(encoderblocks[4],self.channels[3],self.channels[4],True)\n",
    "            ])\n",
    "            self.upsamplers = torch.nn.ModuleList([\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(self.channels[4],self.channels[3],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                ),\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(self.channels[3],self.channels[2],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                ),\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(self.channels[2],self.channels[1],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                ),\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(self.channels[1],self.channels[0],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                )\n",
    "            ])\n",
    "            self.decoderblocks = torch.nn.ModuleList([\n",
    "                ConvBlock(decoderblocks[0],self.channels[4],self.channels[3]),\n",
    "                ConvBlock(decoderblocks[1],self.channels[3],self.channels[2]),\n",
    "                ConvBlock(decoderblocks[2],self.channels[2],self.channels[1]),\n",
    "                ConvBlock(decoderblocks[3],self.channels[1],self.channels[0])\n",
    "            ])\n",
    "        self.classifier = torch.nn.Conv2d(self.channels[0],classes,kernel_size=(1,1),padding='same')\n",
    "        self.upsampler = torch.nn.Upsample(scale_factor=upsample_output, mode='bilinear') if upsample_output != 1 else torch.nn.Identity()\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoderblocks[0](x)\n",
    "        x2 = self.encoderblocks[1](x1)\n",
    "        x3 = self.encoderblocks[2](x2)\n",
    "        x4 = self.encoderblocks[3](x3)\n",
    "        x5 = self.encoderblocks[4](x4)\n",
    "        x4_2 = self.upsamplers[0](x5)\n",
    "        x4_2 = torch.cat((x4,x4_2),1)\n",
    "        x4_2 = self.decoderblocks[0](x4_2)\n",
    "        x3_2 = self.upsamplers[1](x4_2)\n",
    "        x3_2 = torch.cat((x3,x3_2),1)\n",
    "        x3_2 = self.decoderblocks[1](x3_2)\n",
    "        x2_2 = self.upsamplers[2](x3_2)\n",
    "        x2_2 = torch.cat((x2,x2_2),1)\n",
    "        x2_2 = self.decoderblocks[2](x2_2)\n",
    "        x1_2 = self.upsamplers[3](x2_2)\n",
    "        x1_2 = torch.cat((x1,x1_2),1)\n",
    "        x1_2 = self.decoderblocks[3](x1_2)\n",
    "        y = self.classifier(x1_2)\n",
    "        y = self.upsampler(y)\n",
    "        return y\n",
    "class UNetPPM(UNet):\n",
    "    def __init__(self,in_channels,classes,convblocks=None,encoderblocks=[2,2,2,2,2],decoderblocks=[2,2,2,2],upsample_output=1,pools=[1,2,4,8]):\n",
    "        super().__init__(in_channels,classes,convblocks,encoderblocks,decoderblocks,upsample_output)\n",
    "        self.decoderblocks[3] = torch.nn.Sequential(\n",
    "            PPM(self.channels[1],pools),\n",
    "            ConvBlock(decoderblocks[3],(self.channels[1]//4)*len(pools)+self.channels[1],self.channels[0]),\n",
    "        )\n",
    "        #self.classifier = torch.nn.Conv2d(self.channels[0],CLASSES,kernel_size=(1,1),padding='same')\n",
    "class UNetASPP(UNet):\n",
    "    def __init__(self,in_channels,classes,convblocks=None,encoderblocks=[2,2,2,2,2],decoderblocks=[2,2,2,2],upsample_output=1,dilations=[6,12,18]):\n",
    "        super().__init__(in_channels,classes,convblocks,encoderblocks,decoderblocks,upsample_output)\n",
    "        self.decoderblocks[3] = torch.nn.Sequential(\n",
    "            #ASPP(self.channels[1],self.channels[0],dilations),\n",
    "            ConvBlock(decoderblocks[3],self.channels[1],self.channels[0]),\n",
    "            ASPP(self.channels[0],self.channels[0],dilations),\n",
    "        )\n",
    "class SegNet(torch.nn.Module):\n",
    "    def __init__(self,in_channels,classes,convblocks=None,blocks=[2,2,3,3,3]):\n",
    "        super().__init__()\n",
    "        self.channels = [64,128,256,512,512]\n",
    "        if type(convblocks) is torch.nn.ModuleList:\n",
    "            self.encoderblocks = torch.nn.ModuleList([\n",
    "                convblocks[0],\n",
    "                convblocks[1],\n",
    "                convblocks[2],\n",
    "                convblocks[3],\n",
    "                convblocks[4]\n",
    "            ])\n",
    "        else:\n",
    "            self.encoderblocks = torch.nn.ModuleList([\n",
    "                ConvBlock(blocks[0],in_channels,self.channels[0]),\n",
    "                ConvBlock(blocks[1],self.channels[0],self.channels[1]),\n",
    "                ConvBlock(blocks[2],self.channels[1],self.channels[2]),\n",
    "                ConvBlock(blocks[3],self.channels[2],self.channels[3]),\n",
    "                ConvBlock(blocks[4],self.channels[3],self.channels[4])\n",
    "            ])\n",
    "        self.downsamplers = torch.nn.ModuleList([\n",
    "            torch.nn.MaxPool2d(2, stride=2,return_indices=True),\n",
    "            torch.nn.MaxPool2d(2, stride=2,return_indices=True),\n",
    "            torch.nn.MaxPool2d(2, stride=2,return_indices=True),\n",
    "            torch.nn.MaxPool2d(2, stride=2,return_indices=True),\n",
    "            torch.nn.MaxPool2d(2, stride=2,return_indices=True)\n",
    "        ])\n",
    "        self.upsamplers = torch.nn.ModuleList([\n",
    "            torch.nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        ])\n",
    "        self.decoderblocks = torch.nn.ModuleList([\n",
    "            ConvBlock(blocks[4],self.channels[4],self.channels[3],reversed=True),\n",
    "            ConvBlock(blocks[3],self.channels[3],self.channels[2],reversed=True),\n",
    "            ConvBlock(blocks[2],self.channels[2],self.channels[1],reversed=True),\n",
    "            ConvBlock(blocks[1],self.channels[1],self.channels[0],reversed=True),\n",
    "            ConvBlock(blocks[0],self.channels[0],classes,reversed=True)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        x = self.encoderblocks[0](x)\n",
    "        x, i1 = self.downsamplers[0](x)\n",
    "        x = self.encoderblocks[1](x)\n",
    "        x, i2 = self.downsamplers[0](x)\n",
    "        x = self.encoderblocks[2](x)\n",
    "        x, i3 = self.downsamplers[0](x)\n",
    "        x = self.encoderblocks[3](x)\n",
    "        x, i4 = self.downsamplers[0](x)\n",
    "        x = self.encoderblocks[4](x)\n",
    "        x, i5 = self.downsamplers[0](x)\n",
    "        x = self.upsamplers[0](x, indices=i5)\n",
    "        x = self.decoderblocks[0](x)\n",
    "        x = self.upsamplers[1](x, indices=i4)\n",
    "        x = self.decoderblocks[1](x)\n",
    "        x = self.upsamplers[2](x, indices=i3)\n",
    "        x = self.decoderblocks[2](x)\n",
    "        x = self.upsamplers[3](x, indices=i2)\n",
    "        x = self.decoderblocks[3](x)\n",
    "        x = self.upsamplers[4](x, indices=i1)\n",
    "        x = self.decoderblocks[4](x)\n",
    "        return x\n",
    "class FastSCNN(torch.nn.Module):\n",
    "    def __init__(self,in_channels,classes,backbonemodel=None):\n",
    "        super().__init__()\n",
    "        self.downsample = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3,32,kernel_size=(3,3),padding=1,stride=2,bias=False), #Bias is pointless for BatchNorm\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32,32,kernel_size=(3,3),padding=1,groups=32,stride=2,bias=False), #Depthwise Conv\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32,48,kernel_size=(1,1),padding='same',bias=False), #Pointwise Conv\n",
    "            torch.nn.BatchNorm2d(48),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(48,48,kernel_size=(3,3),padding=1,groups=48,stride=2,bias=False), #Depthwise Conv\n",
    "            torch.nn.BatchNorm2d(48),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(48,64,kernel_size=(1,1),padding='same',bias=False), #Pointwise Conv\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.bottlenecks = torch.nn.Sequential(\n",
    "            InvertedBottleResBlock(64,6,64,2),\n",
    "            InvertedBottleResBlock(64,6,64,1),\n",
    "            InvertedBottleResBlock(64,6,64,1),\n",
    "            InvertedBottleResBlock(64,6,96,2),\n",
    "            InvertedBottleResBlock(96,6,96,1),\n",
    "            InvertedBottleResBlock(96,6,96,1),\n",
    "            InvertedBottleResBlock(96,6,128,1),\n",
    "            InvertedBottleResBlock(128,6,128,1),\n",
    "            InvertedBottleResBlock(128,6,128,1)\n",
    "        )\n",
    "        self.ppm = PPM(128) #outchannel is 256\n",
    "        self.fuse1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64,128,kernel_size=(1,1),padding='valid',bias=False), #Bias is pointless for BatchNorm\n",
    "            torch.nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.fuse2 = torch.nn.Sequential(\n",
    "            torch.nn.Upsample(scale_factor=4, mode='bilinear'),\n",
    "            torch.nn.Conv2d(256,256,kernel_size=(3,3),padding='same',groups=256,bias=False), #DWConv\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(256,128,kernel_size=(1,1),padding='valid',bias=False),\n",
    "            torch.nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.fuse = torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.last = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128,128,kernel_size=(3,3),padding='same',groups=128,bias=False), #Depthwise Conv\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128,128,kernel_size=(1,1),padding='same',bias=False), #Pointwise Conv\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128,128,kernel_size=(3,3),padding='same',groups=128,bias=False), #Depthwise Conv\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128,128,kernel_size=(1,1),padding='same',bias=False), #Pointwise Conv\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128,classes,kernel_size=(1,1),padding='same',bias=False), #Pointwise Conv\n",
    "            torch.nn.Upsample(scale_factor=8, mode='bilinear')\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.downsample(x)\n",
    "        x2 = self.bottlenecks(x)\n",
    "        x2 = self.ppm(x2)\n",
    "        x1 = self.fuse1(x)\n",
    "        x2 = self.fuse2(x2)\n",
    "        x = torch.add(x1,x2)\n",
    "        x = self.fuse(x)\n",
    "        x = self.last(x)\n",
    "        return x\n",
    "def INF(B,H,W):\n",
    "    return -torch.diag(torch.tensor(float(\"inf\")).cuda().repeat(H),0).unsqueeze(0).repeat(B*W,1,1)\n",
    "class CrissCrossAttention(torch.nn.Module):\n",
    "    \"\"\" Criss-Cross Attention Module, copied straight from the repo\"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(CrissCrossAttention,self).__init__()\n",
    "        \n",
    "        self.query_conv = torch.nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.key_conv = torch.nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.value_conv = torch.nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.softmax = torch.nn.Softmax(dim=3)\n",
    "        self.INF = INF\n",
    "        self.gamma = torch.nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x):\n",
    "        m_batchsize, _, height, width = x.size()\n",
    "        proj_query = self.query_conv(x)\n",
    "        proj_query_H = proj_query.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height).permute(0, 2, 1)\n",
    "        proj_query_W = proj_query.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x)\n",
    "        proj_key_H = proj_key.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height)\n",
    "        proj_key_W = proj_key.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width)\n",
    "        proj_value = self.value_conv(x)\n",
    "        proj_value_H = proj_value.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height)\n",
    "        proj_value_W = proj_value.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width)\n",
    "        energy_H = (torch.bmm(proj_query_H, proj_key_H)+self.INF(m_batchsize, height, width)).view(m_batchsize,width,height,height).permute(0,2,1,3)\n",
    "        energy_W = torch.bmm(proj_query_W, proj_key_W).view(m_batchsize,height,width,width)\n",
    "        concate = self.softmax(torch.cat([energy_H, energy_W], 3))\n",
    "        att_H = concate[:,:,:,0:height].permute(0,2,1,3).contiguous().view(m_batchsize*width,height,height)\n",
    "        #print(concate)\n",
    "        #print(att_H)\n",
    "        att_W = concate[:,:,:,height:height+width].contiguous().view(m_batchsize*height,width,width)\n",
    "        out_H = torch.bmm(proj_value_H, att_H.permute(0, 2, 1)).view(m_batchsize,width,-1,height).permute(0,2,3,1)\n",
    "        out_W = torch.bmm(proj_value_W, att_W.permute(0, 2, 1)).view(m_batchsize,height,-1,width).permute(0,2,1,3)\n",
    "        #print(out_H.size(),out_W.size())\n",
    "        return self.gamma*(out_H + out_W) + x\n",
    "class RCCAModule(torch.nn.Module):\n",
    "    \"\"\"Also copied too\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, num_classes):\n",
    "        super(RCCAModule, self).__init__()\n",
    "        inter_channels = in_channels // 4\n",
    "        self.conva = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(inter_channels),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.cca = CrissCrossAttention(inter_channels)\n",
    "        self.convb = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(inter_channels),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.bottleneck = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels+inter_channels, out_channels, kernel_size=3, padding=1, dilation=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout2d(0.1),\n",
    "            torch.nn.Conv2d(512, num_classes, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        )\n",
    "    def forward(self, x, recurrence=1):\n",
    "        output = self.conva(x)\n",
    "        for i in range(recurrence):\n",
    "            output = self.cca(output)\n",
    "        output = self.convb(output)\n",
    "        output = self.bottleneck(torch.cat([x, output], 1))\n",
    "        return output\n",
    "class DilatedResNet(torch.nn.Module):\n",
    "    def __init__(self,in_channels,classes,layers=[3,4,6,3]):\n",
    "        super().__init__()\n",
    "        def ResNetBlock(num,channels,bottlechan,filters,stride=1,dilation=1,addpool=False):\n",
    "            blocks = torch.nn.Sequential()\n",
    "            if addpool:\n",
    "                blocks.append(torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "            for i in range(num):\n",
    "                if stride != 1 and i == 0:\n",
    "                    blocks.append(BottleResBlock(channels,bottlechan,filters,stride,dilation=dilation))\n",
    "                else:\n",
    "                    blocks.append(BottleResBlock(channels,bottlechan,filters,dilation=dilation))\n",
    "                channels = filters\n",
    "            return blocks\n",
    "        self.blocks = torch.nn.Sequential(\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels,64,kernel_size=(7,7),stride=2,padding=3,bias=False),\n",
    "                torch.nn.BatchNorm2d(64),\n",
    "                torch.nn.ReLU(),\n",
    "            ),\n",
    "            ResNetBlock(layers[0],64,64,256,addpool=True),\n",
    "            ResNetBlock(layers[1],256,128,512,stride=2),\n",
    "            ResNetBlock(layers[2],512,256,1024,stride=1,dilation=2),\n",
    "            ResNetBlock(layers[3],1024,512,2048,stride=1,dilation=4),\n",
    "        )\n",
    "        self.head = torch.nn.Conv2d(2048,classes,kernel_size=(1,1),padding='same')\n",
    "        self.upsampler = torch.nn.Upsample(scale_factor=8, mode='bilinear')\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        y = self.head(x)\n",
    "        y = self.upsampler(y)\n",
    "        return y\n",
    "class CCNet(DilatedResNet):\n",
    "    def __init__(self,in_channels,classes,layers=[3,4,6,3],recurrence=2):\n",
    "        super().__init__(in_channels,classes,layers=[3,4,6,3])\n",
    "        self.head = RCCAModule(2048, 512, classes)\n",
    "        self.upsampler = torch.nn.Upsample(scale_factor=8, mode='bilinear')\n",
    "        self.recur = recurrence\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        y = self.head(x,self.recur)\n",
    "        y = self.upsampler(y)\n",
    "        return y\n",
    "class DilatedResNetWithPPM(DilatedResNet):\n",
    "    def __init__(self,in_channels,classes,layers=[3,4,6,3]):\n",
    "        super().__init__(in_channels,classes,layers=[3,4,6,3])\n",
    "        self.ppm = PPM(2048)\n",
    "        self.head = torch.nn.Conv2d((2048//4)*4+2048,classes,kernel_size=(1,1),padding='same')\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        x = self.ppm(x)\n",
    "        y = self.head(x)\n",
    "        y = self.upsampler(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee44cf-f0a0-414c-8321-84615c786039",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52747a-f0ed-40be-afb7-2a5f7ab84e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(pred, target, n_classes = 2, include_bg_class = True):\n",
    "    ious = []\n",
    "    # Assuming the shapes are BATCH x 1 x H x W => BATCH x H x W\n",
    "    pred = pred.cpu()\n",
    "    target = target.cpu()\n",
    "    for cls in range(0 if include_bg_class else 1, n_classes):  # This goes from 1:n_classes-1 -> class \"0\" is ignored\n",
    "        pred2 = pred == cls\n",
    "        target2 = target == cls\n",
    "        intersection = torch.logical_and(pred2,target2).count_nonzero((1,2)).data.cpu().numpy()\n",
    "        #union = torch.logical_or(pred2,target2).count_nonzero((1,2)).data.cpu().item()\n",
    "        union = pred2.count_nonzero((1,2)).data.cpu().numpy() + target2.count_nonzero((1,2)).data.cpu().numpy() - intersection\n",
    "        ious.append((intersection/union))\n",
    "    return ious\n",
    "class EarlyStopper:\n",
    "    def __init__(self, metric_name='val_loss', lower_is_better=True, patience=4, delta=0):\n",
    "        self.metric_name = metric_name\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.lower_is_better = lower_is_better\n",
    "        if self.lower_is_better:\n",
    "            self.best_metric = float('inf')\n",
    "        else:\n",
    "            self.best_metric = -float('inf')\n",
    "    def early_stop(self, history, lower_is_better=True):\n",
    "        metric = history[self.metric_name][-1]\n",
    "        if self.lower_is_better:\n",
    "            if metric < self.best_metric:\n",
    "                self.best_metric = metric\n",
    "                self.counter = 0\n",
    "                return -1\n",
    "            elif metric > (self.best_metric + self.delta):\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    return 1\n",
    "            return 0\n",
    "        else:\n",
    "            if metric > self.best_metric:\n",
    "                self.best_metric = metric\n",
    "                self.counter = 0\n",
    "                return -1\n",
    "            elif metric < (self.best_metric + self.delta):\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    return 1\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808614b7-cdf1-492b-817e-2cd6a0a5530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(y):\n",
    "    return torch.nn.functional.one_hot(y,7).float() #cursed\n",
    "class Manager():\n",
    "    def __init__(self, train_dl, val_dl, model, loss_fn, optimizer, preprocesser = None, scheduler = None):\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.preprocesser = preprocesser\n",
    "        self.scheduler = scheduler\n",
    "        parametertensor = next(self.model.parameters())\n",
    "        self.to_device = parametertensor.device\n",
    "        self.rescaler = torchvision.transforms.v2.ToDtype(parametertensor.dtype,True) #infer the dtype of input\n",
    "        self.stopnow = False\n",
    "        self.personalbest = False\n",
    "        self.pbrecord = None\n",
    "        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'best_epoch': 0}\n",
    "    def train_one_epoch(self):\n",
    "        size = len(self.train_dl.dataset)\n",
    "        count, count_true = 0, 0\n",
    "        batchsizes, losses = [], []\n",
    "        self.model.train()\n",
    "        for X, y in self.train_dl:\n",
    "            #preprocess layers here\n",
    "            X = self.rescaler(X).to(device=self.to_device,memory_format=torch.channels_last)\n",
    "            #y = y.to(dtype=torch.int64)\n",
    "            y = y.to(device=self.to_device)\n",
    "            if self.preprocesser != None:\n",
    "                X = self.preprocesser(X)\n",
    "            # Compute prediction and loss\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            # Stop propagating gradient\n",
    "            loss = loss.item()\n",
    "            # and trues\n",
    "            pred = pred.argmax(1)\n",
    "            count_true += (pred==y.argmax(1)).count_nonzero().item()\n",
    "            #\n",
    "            batchsizes.append(len(X))\n",
    "            losses.append(loss)\n",
    "            count += len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{count:>5d}/{size:>5d}]\",end = '\\r')\n",
    "        print()\n",
    "        meanloss = np.average(np.array(losses),weights=np.array(batchsizes))\n",
    "        accuracy = count_true/count\n",
    "        print(f\"Training Error:   Accuracy: {(100*accuracy):>.02f}%, Avg loss: {meanloss:>8f}\")\n",
    "        self.history['train_loss'].append(meanloss)\n",
    "        self.history['train_acc'].append(accuracy)\n",
    "    def validate(self, earlystop):\n",
    "        size = len(self.val_dl.dataset)\n",
    "        self.model.eval()\n",
    "        num_batches = len(self.val_dl)\n",
    "        count, count_true = 0, 0\n",
    "        batchsizes, losses = [], []\n",
    "        with torch.no_grad(): # reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "            for X, y in self.val_dl:\n",
    "                X = self.rescaler(X).to(device=self.to_device,memory_format=torch.channels_last)\n",
    "                y = y.to(device=self.to_device,dtype=torch.int64)\n",
    "                pred = self.model(X)\n",
    "                val_loss = self.loss_fn(pred, y).item()\n",
    "                # and trues\n",
    "                pred = pred.argmax(1)\n",
    "                count_true += (pred==y).count_nonzero().item()\n",
    "                #\n",
    "                batchsizes.append(len(X))\n",
    "                losses.append(val_loss)\n",
    "                count += len(X)\n",
    "        meanloss = np.average(np.array(losses),weights=np.array(batchsizes))\n",
    "        accuracy = count_true/count\n",
    "        print(f\"Validating Error: Accuracy: {(100*accuracy):>.02f}%, Avg loss: {meanloss:>8f}\")\n",
    "        self.history['val_loss'].append(meanloss)\n",
    "        self.history['val_acc'].append(accuracy)\n",
    "        earlystopout = earlystop.early_stop(self.history)\n",
    "        self.stopnow = earlystopout == 1\n",
    "        self.personalbest = earlystopout == -1\n",
    "    def evaluate(self, dataloader):\n",
    "        size = len(dataloader.dataset)\n",
    "        self.model.eval()\n",
    "        num_batches = len(dataloader)\n",
    "        count, count_true = 0, 0\n",
    "        batchsizes, losses, mious = [], [], []\n",
    "        with torch.no_grad(): # reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "            for X, y in dataloader:\n",
    "                X = self.rescaler(X).to(device=self.to_device,memory_format=torch.channels_last)\n",
    "                y = y.to(device=self.to_device,dtype=torch.int64)\n",
    "                pred = self.model(X)\n",
    "                loss = self.loss_fn(pred, y).item()\n",
    "                # and trues\n",
    "                pred = pred.argmax(1)\n",
    "                count_true += (pred==y).count_nonzero().item()\n",
    "                #\n",
    "                batchsizes.append(len(X))\n",
    "                losses.append(loss)\n",
    "                count += len(X)\n",
    "        meanloss = np.average(np.array(losses),weights=np.array(batchsizes))\n",
    "        accuracy = count_true/count\n",
    "        return {'loss': meanloss, 'acc': accuracy}\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X = self.rescaler(X.unsqueeze(0)).to(device=self.to_device,memory_format=torch.channels_last)\n",
    "            pred = self.model(X)\n",
    "        return pred.detach()\n",
    "    def train(self, epochs, earlystop, restore_best_weights = True):\n",
    "        epoch = 0\n",
    "        while epoch != epochs:\n",
    "            if self.scheduler != None:\n",
    "                print(f\"Epoch {epoch+1} ({self.scheduler.get_last_lr()})\\n-------------------------------\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "            #with torch.autograd.detect_anomaly():\n",
    "            self.train_one_epoch()\n",
    "            self.validate(earlystop)\n",
    "            if self.scheduler != None:\n",
    "                if type(self.scheduler) is torch.optim.lr_scheduler.ReduceLROnPlateau:\n",
    "                    self.scheduler.step(self.history['val_loss'][epoch])\n",
    "                else:\n",
    "                    self.scheduler.step()\n",
    "            if self.personalbest:\n",
    "                print('New personal best!')\n",
    "                if restore_best_weights:\n",
    "                    #torch.save(model.state_dict(),'temp-personalbest.pth')\n",
    "                    self.pbrecord = copy.deepcopy(model.state_dict())\n",
    "                self.history['best_epoch'] = epoch #zero-indexed\n",
    "            if self.stopnow:\n",
    "                print('Stop.')\n",
    "                print()\n",
    "                break\n",
    "            print()\n",
    "            epoch += 1\n",
    "        print('Training ended.')\n",
    "        if restore_best_weights:\n",
    "            print('Restoring best weights...')\n",
    "            #self.model.load_state_dict(torch.load('temp-personalbest.pth'))\n",
    "            self.model.load_state_dict(self.pbrecord)\n",
    "        return self.history\n",
    "def dice_loss(pred, target):\n",
    "    smooth = 1.\n",
    "    num = pred.size(0)\n",
    "    m1 = pred.contiguous().view(num, -1).float()  # Flatten\n",
    "    m2 = target.contiguous().view(num, -1).float()  # Flatten\n",
    "    intersection = (m1 * m2).sum().float()\n",
    "    dice = (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)\n",
    "    return 1-dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a578a8c-5eb2-4c19-9c6e-c7451cd50821",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a98b63-6b86-497c-9bbf-9100a7c5f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lazies(model):\n",
    "    # Initialize lazy modules by passing at least something\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model(torch.rand(1, 3, HEIGHT, WIDTH).to(device=DEVICE,dtype=DTYPE))\n",
    "def init_weights(m):\n",
    "    print(type(m))\n",
    "    if type(m) is torch.nn.Linear or type(m) is torch.nn.LazyLinear\\\n",
    "    or type(m) is torch.nn.Conv2d or type(m) is torch.nn.LazyConv2d\\\n",
    "    or type(m) is torch.nn.ConvTranspose2d or type(m) is torch.nn.LazyConvTranspose2d:\n",
    "        if type(m.weight) is torch.nn.UninitializedParameter:\n",
    "            print(\"Skipping uninitialized weights....\")\n",
    "            assert(False)\n",
    "        print(\"Shape: \"+str(m.weight.shape)+\" + \"+str(m.bias.shape if m.bias is not None else \"\"))\n",
    "        if not m.weight.requires_grad:\n",
    "            print(\"Weights frozen, skipping....\")\n",
    "        elif len(m.weight.shape) > 1:\n",
    "            #torch.nn.init.xavier_normal_(m.weight, gain=1.4142135623730950488016887242097)\n",
    "            torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            #torch.nn.init.normal_(m.weight,0,0.01)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "            print(\"Applied He initializer!\")\n",
    "        else:\n",
    "            print(\"He initializer not applicable for 1 dimensional tensor.\")\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size(),device=tensor.device) * self.std + self.mean\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "class MakeOHE(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, tensor, tensor2):\n",
    "        return (tensor, ohe(tensor2))\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d8610d-966a-454d-bdaa-57b4f50c1605",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a7137-3869-45f1-a465-ed610a2442b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = r'C:\\Users\\User\\Documents\\AAAAA\\eseg2\\SB-FishDisease-P20-20-EP20-splitAA'\n",
    "DATASETNAME = 'Fishv2_CLS_DE_AA'\n",
    "train_img_dir = dataset_dir+r'\\train'\n",
    "val_img_dir = dataset_dir+r'\\valid'\n",
    "test_img_dir = dataset_dir+r'\\test'\n",
    "\n",
    "print(torch.hub.list('pytorch/vision'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd66b39b-3375-4236-9a2a-3f186dfeb560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DTYPE = torch.float32\n",
    "EPOCHS = 2000\n",
    "RBW = True\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "HEIGHT, WIDTH = 448, 448\n",
    "\n",
    "scheduler = None\n",
    "\n",
    "''' Model selection:\n",
    "VGG13\n",
    "PRE_VGG13\n",
    "VGG16\n",
    "PRE_VGG16\n",
    "PRE_ResNet18-test\n",
    "PRE_ResNet34\n",
    "ResNet50\n",
    "PRE_ResNet50\n",
    "ResNet101\n",
    "PRE_ResNet101\n",
    "'''\n",
    "\n",
    "MODELNAME = 'PRE_VGG16'\n",
    "\n",
    "match MODELNAME:\n",
    "    case 'VGG16':\n",
    "        model = torch.hub.load('pytorch/vision', 'vgg16_bn', weights=None)\n",
    "        model.classifier[6] = torch.nn.Linear(4096,7)\n",
    "        model.apply(init_weights)\n",
    "        model = model.to(device=DEVICE,dtype=DTYPE)\n",
    "        model.requires_grad_(True)\n",
    "        antilatestopper = EarlyStopper('val_loss',lower_is_better=True,patience=100)\n",
    "    case 'PRE_VGG16':\n",
    "        model = torch.hub.load('pytorch/vision', 'vgg16_bn', weights=\"VGG16_BN_Weights.IMAGENET1K_V1\")\n",
    "        model.classifier[6] = torch.nn.Linear(4096,7)\n",
    "        init_weights(model.classifier[6])\n",
    "        model = model.to(device=DEVICE,dtype=DTYPE)\n",
    "        model.requires_grad_(True)\n",
    "        antilatestopper = EarlyStopper('val_loss',lower_is_better=True,patience=10) ######\n",
    "    case 'PRE_ResNet18-test':\n",
    "        BATCH_SIZE = 43\n",
    "        model = torch.hub.load('pytorch/vision', 'resnet18', weights='ResNet18_Weights.IMAGENET1K_V1')\n",
    "        model.fc = torch.nn.Linear(512,7)\n",
    "        init_weights(model.fc)\n",
    "        model = model.to(device=DEVICE,dtype=DTYPE)\n",
    "        model.requires_grad_(True)\n",
    "        antilatestopper = EarlyStopper('val_loss',lower_is_better=True,patience=10) ###\n",
    "    case 'PRE_ResNet34':\n",
    "        model = torch.hub.load('pytorch/vision', 'resnet34', weights='ResNet34_Weights.IMAGENET1K_V1')\n",
    "        model.fc = torch.nn.Linear(512,7)\n",
    "        init_weights(model.fc)\n",
    "        model = model.to(device=DEVICE,dtype=DTYPE)\n",
    "        model.requires_grad_(True)\n",
    "        antilatestopper = EarlyStopper('val_loss',lower_is_better=True,patience=10)\n",
    "    case 'ResNet50':\n",
    "        model = torch.hub.load('pytorch/vision', 'resnet50', weights=None)\n",
    "        model.fc = torch.nn.Linear(2048,7)\n",
    "        model.apply(init_weights)\n",
    "        model = model.to(device=DEVICE,dtype=DTYPE)\n",
    "        model.requires_grad_(True)\n",
    "        antilatestopper = EarlyStopper('val_loss',lower_is_better=True,patience=100)\n",
    "    case 'PRE_ResNet50':\n",
    "        model = torch.hub.load('pytorch/vision', 'resnet50', weights='ResNet50_Weights.IMAGENET1K_V2')\n",
    "        model.fc = torch.nn.Linear(2048,7)\n",
    "        init_weights(model.fc)\n",
    "        model = model.to(device=DEVICE,dtype=DTYPE)\n",
    "        model.requires_grad_(True)\n",
    "        antilatestopper = EarlyStopper('val_loss',lower_is_better=True,patience=10)\n",
    "    case 'ResNet101':\n",
    "        model = torch.hub.load('pytorch/vision', 'resnet101', weights=None)\n",
    "        model.fc = torch.nn.Linear(2048,7)\n",
    "        model.apply(init_weights)\n",
    "        model = model.to(device=DEVICE,dtype=DTYPE)\n",
    "        model.requires_grad_(True)\n",
    "        antilatestopper = EarlyStopper('val_loss',lower_is_better=True,patience=100)\n",
    "    case 'PRE_ResNet101':\n",
    "        model = torch.hub.load('pytorch/vision', 'resnet101', weights='ResNet101_Weights.IMAGENET1K_V2')\n",
    "        model.fc = torch.nn.Linear(2048,7)\n",
    "        init_weights(model.fc)\n",
    "        model = model.to(device=DEVICE,dtype=DTYPE)\n",
    "        model.requires_grad_(True)\n",
    "        antilatestopper = EarlyStopper('val_loss',lower_is_better=True,patience=10)\n",
    "    case _:\n",
    "        assert(False)\n",
    "\n",
    "'''Optimizer selection:\n",
    "SGD_1e-4_0,9 #VGG16 -> Good\n",
    "SGD_3e-4_0,9\n",
    "SGD_3e-5_0,9\n",
    "SGD_1e-3_0,9_1e-4 #ResNet -> Good!\n",
    "SGD_1e-4_0,9_1e-5\n",
    "SGD_3e-5_0,9_1e-5\n",
    "SGD_1e-3U_0,9_1e-4\n",
    "SGD_1e-3_0,9_1e-5\n",
    "Adam_5e-5\n",
    "Adam_1e-5\n",
    "'''\n",
    "\n",
    "OPTIMIZERNAME = 'SGD_1e-4_0,9'\n",
    "\n",
    "match OPTIMIZERNAME:\n",
    "    case 'SGD_1e-4_0,9':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=0)\n",
    "    case 'SGD_3e-4_0,9':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=3e-4, momentum=0.9, weight_decay=0)\n",
    "    case 'SGD_3e-5_0,9':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=3e-5, momentum=0.9, weight_decay=0)\n",
    "    case 'SGD_1e-3_0,9_1e-4':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-4)\n",
    "    case 'SGD_1e-4_0,9_1e-5':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=1e-5)\n",
    "    case 'SGD_3e-5_0,9_1e-5':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=3e-5, momentum=0.9, weight_decay=1e-5)\n",
    "    case 'SGD_1e-3U_0,9_1e-4':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.316227766, patience=5, threshold=0, cooldown=0, min_lr=1e-4)\n",
    "        scheduler.cooldown_counter = 5\n",
    "    case 'SGD_1e-3_0,9_1e-5':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-5)\n",
    "    case 'Adam_5e-5':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, betas=(0.9, 0.999), weight_decay=0)\n",
    "    case 'Adam_1e-5':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.999), weight_decay=0)\n",
    "    case 'SGD_1e-4_0,9':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=0)\n",
    "    case _:\n",
    "        assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c0f68c-5904-4d70-b5db-ed0d1ba14623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds = ImageGenerator(train_img_dir,size=(HEIGHT,WIDTH))\n",
    "val_ds = ImageGenerator(val_img_dir,size=(HEIGHT,WIDTH))\n",
    "test_ds = ImageGenerator(test_img_dir,size=(HEIGHT,WIDTH))\n",
    "\n",
    "PREPREPROCESSOR = 'CutMixUp' #MixUp > CutMix > None\n",
    "\n",
    "match PREPREPROCESSOR:\n",
    "    case 'None':\n",
    "        pretransform = torchvision.transforms.v2.Compose([\n",
    "            torchvision.transforms.v2.ToDtype(torch.float32, True),\n",
    "            MakeOHE()\n",
    "        ])\n",
    "    case 'MixUp':\n",
    "        pretransform = torchvision.transforms.v2.Compose([\n",
    "            torchvision.transforms.v2.ToDtype(torch.float32, True),\n",
    "            torchvision.transforms.v2.RandomChoice([\n",
    "                MakeOHE(),\n",
    "                torchvision.transforms.v2.MixUp(num_classes=7)\n",
    "            ]) # ,p=[0.9,0.1]\n",
    "        ])\n",
    "    case 'CutMix':\n",
    "        pretransform = torchvision.transforms.v2.Compose([\n",
    "            torchvision.transforms.v2.ToDtype(torch.float32, True),\n",
    "            torchvision.transforms.v2.RandomChoice([\n",
    "                MakeOHE(),\n",
    "                torchvision.transforms.v2.CutMix(num_classes=7)\n",
    "            ]) #\n",
    "        ])\n",
    "    case 'CutMixUp':\n",
    "        pretransform = torchvision.transforms.v2.Compose([\n",
    "            torchvision.transforms.v2.ToDtype(torch.float32, True),\n",
    "            torchvision.transforms.v2.RandomChoice([\n",
    "                MakeOHE(),\n",
    "                torchvision.transforms.v2.MixUp(num_classes=7),\n",
    "                torchvision.transforms.v2.CutMix(num_classes=7)\n",
    "            ])\n",
    "        ])\n",
    "    case _:\n",
    "        assert(False)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return pretransform(*torch.utils.data.default_collate(batch))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=True,pin_memory=True,collate_fn=collate_fn)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_ds,batch_size=BATCH_SIZE,pin_memory=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_ds,batch_size=BATCH_SIZE)\n",
    "\n",
    "print(model)\n",
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43408ce-a209-449d-af18-2e5bb524ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.v2.Compose([\n",
    "    AddGaussianNoise(0,0.0125),\n",
    "    torchvision.transforms.v2.RandomChoice([\n",
    "        torchvision.transforms.v2.RandomAffine(0),\n",
    "        torchvision.transforms.v2.GaussianBlur(5),\n",
    "        torchvision.transforms.v2.GaussianBlur(9),\n",
    "    ]),\n",
    "    torchvision.transforms.v2.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.v2.RandomVerticalFlip(p=0.5),\n",
    "    torchvision.transforms.v2.ColorJitter(0.25,0.0,0.25),\n",
    "    torchvision.transforms.v2.RandomChoice([\n",
    "        torchvision.transforms.v2.RandomAffine(0),\n",
    "        torchvision.transforms.v2.ElasticTransform(alpha=768, sigma=14),\n",
    "        torchvision.transforms.v2.ElasticTransform(alpha=2048, sigma=20)\n",
    "    ]),\n",
    "    torchvision.transforms.v2.RandomRotation((-15,15))\n",
    "]).to(DEVICE)\n",
    "\n",
    "#weight = torch.tensor([23, 20, 15, 24, 90, 21, 20]) #Train only\n",
    "weight = torch.tensor([28, 25, 19, 29, 109, 26, 24]) #Train and Valid\n",
    "#weight = torch.tensor([33, 29, 22, 34, 128, 30, 28]) #Train, Valid, and Test\n",
    "print(weight)\n",
    "weight = np.mean(weight.numpy())/weight\n",
    "print(weight)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weight.to(device=DEVICE,dtype=DTYPE)) #Crossentropy requires logits, no softmax required\n",
    "LOSSNAME = 'CE_wprop'\n",
    "\n",
    "print(scheduler)\n",
    "\n",
    "model_manager = Manager(train_dataloader, val_dataloader, model, loss_fn, optimizer, transforms, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1edc4d-1762-4734-81f6-761c515de804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_history = None\n",
    "model_history = model_manager.train(EPOCHS, antilatestopper, restore_best_weights = RBW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2941d6-17c1-4a8c-9d76-37f39be2d4fe",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00fa748-73cf-42b6-8be2-2a56546f9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_ds,batch_size=BATCH_SIZE)\n",
    "del val_dataloader\n",
    "val_dataloader = torch.utils.data.DataLoader(val_ds,batch_size=BATCH_SIZE)\n",
    "train_metric = model_manager.evaluate(train_dataloader)\n",
    "val_metric = model_manager.evaluate(val_dataloader)\n",
    "test_metric = model_manager.evaluate(test_dataloader)\n",
    "\n",
    "print(f\"Training Error: Accuracy: {(100*train_metric['acc']):>.3f}%, Avg loss: {train_metric['loss']:>8f}\")\n",
    "print(f\"Validating Error: Accuracy: {(100*val_metric['acc']):>.3f}%, Avg loss: {val_metric['loss']:>8f}\")\n",
    "print(f\"Testing Error: Accuracy: {(100*test_metric['acc']):>.3f}%, Avg loss: {test_metric['loss']:>8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643af2cf-cb84-499c-97b0-f5665a4f66da",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metrics = ('{:.2%}'.format(train_metric['acc'])+'_'+'{:.2%}'.format(val_metric['acc'])+'_'+'{:.2%}'.format(test_metric['acc'])).replace('.',',')\n",
    "size = str(WIDTH)+'^2' if WIDTH == HEIGHT else str(WIDTH)+'×'+str(HEIGHT)\n",
    "precision_name = \"-half\" if next(model.parameters()).dtype == torch.float16 else \"-bhalf\" if next(model.parameters()).dtype == torch.bfloat16 else ''\n",
    "NAME = MODELNAME+precision_name+'-'+size+'-'+DATASETNAME+\"+\"+PREPREPROCESSOR+'-'+OPTIMIZERNAME+'-'+LOSSNAME+'-BS'+str(BATCH_SIZE) \\\n",
    "+'-E'+str(model_history['best_epoch'])+'-'+best_metrics\n",
    "print(NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679d3ba-ed34-40fd-a9f7-a65f3e60edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_ds,batch_size=1)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds,batch_size=1)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds,batch_size=1)\n",
    "train_count = np.zeros((7, 7), dtype=int)\n",
    "val_count = np.zeros((7, 7), dtype=int)\n",
    "test_count = np.zeros((7, 7), dtype=int)\n",
    "for i, j in train_dl:\n",
    "    true = j\n",
    "    pred = model_manager.predict(i.squeeze(0)).cpu().argmax(1).item()\n",
    "    train_count[true][pred]+=1\n",
    "for i, j in val_dl:\n",
    "    true = j\n",
    "    pred = model_manager.predict(i.squeeze(0)).cpu().argmax(1).item()\n",
    "    val_count[true][pred]+=1\n",
    "for i, j in test_dl:\n",
    "    true = j\n",
    "    pred = model_manager.predict(i.squeeze(0)).cpu().argmax(1).item()\n",
    "    test_count[true][pred]+=1\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(train_count)\n",
    "for i in range(7):\n",
    "    for j in range(7):\n",
    "        text = plt.text(j, i, train_count[i, j], ha=\"center\", va=\"center\", color=\"w\")\n",
    "plt.title('Train')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Actual')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(val_count)\n",
    "for i in range(7):\n",
    "    for j in range(7):\n",
    "        text = plt.text(j, i, val_count[i, j], ha=\"center\", va=\"center\", color=\"w\")\n",
    "plt.title('Valid')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Actual')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(test_count)\n",
    "for i in range(7):\n",
    "    for j in range(7):\n",
    "        text = plt.text(j, i, test_count[i, j], ha=\"center\", va=\"center\", color=\"w\")\n",
    "plt.title('Test')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Actual')\n",
    "plt.savefig(NAME+'-Result') #save before show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774727cc-c58e-4bea-af17-80a73672b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = model_history['train_loss']\n",
    "val_loss = model_history['val_loss']\n",
    "train_acc = model_history['train_acc']\n",
    "val_acc = model_history['val_acc']\n",
    "best_epoch = model_history['best_epoch']\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(len(train_loss)), train_loss, '-r', label='Training')\n",
    "plt.plot(range(len(val_loss)), val_loss, '-b', label='Validation')\n",
    "plt.axvline(x=best_epoch)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.ylim([0, 2.5])\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(train_acc)), train_acc, '-r', label='Training')\n",
    "plt.plot(range(len(val_acc)), val_acc, '-b', label='Validation')\n",
    "plt.axvline(x=best_epoch)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.savefig(NAME+'-TrainPlot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babfd7e-097c-4157-9cd9-2b463887b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, NAME+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9def3ad-30a7-4ee3-863b-053c4f23a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Log-CLS.txt\", \"a\") as f:\n",
    "    f.write('Name: ')\n",
    "    f.write(NAME)\n",
    "    f.write('\\n')\n",
    "    f.write(str(loss_fn))\n",
    "    f.write('\\n')\n",
    "    f.write(str(optimizer))\n",
    "    f.write('\\n')\n",
    "    f.write(str(transforms))\n",
    "    f.write('\\n')\n",
    "    f.write('History: (Train Loss, Val Loss, Train Accuracy, Val Accuracy)\\n')\n",
    "    for i in train_loss:\n",
    "        f.write(str(i))\n",
    "        f.write('\\t')\n",
    "    f.write('\\n')\n",
    "    for i in val_loss:\n",
    "        f.write(str(i))\n",
    "        f.write('\\t')\n",
    "    f.write('\\n')\n",
    "    for i in train_acc:\n",
    "        f.write(str(i))\n",
    "        f.write('\\t')\n",
    "    f.write('\\n')\n",
    "    for i in val_acc:\n",
    "        f.write(str(i))\n",
    "        f.write('\\t')\n",
    "    f.write('\\n')\n",
    "    f.write('-------------------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
