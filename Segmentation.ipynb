{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79330542-c9bd-42d4-afe7-16cc5b8bd93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "import copy\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    "DEVICE = torch.device(dev)\n",
    "\n",
    "import sys\n",
    "ARGNAME = sys.argv[-1]\n",
    "print(ARGNAME)\n",
    "if not ARGNAME.startswith('C:\\\\'):\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5de9b-a7b5-41b7-b9d6-ad774e9fd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgdir, cocofilename=r'_annotations.coco.json', size=None):\n",
    "        \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "          Args:\n",
    "            data: Data containing video file paths.\n",
    "        \"\"\"\n",
    "        self.imgdir = imgdir\n",
    "        self.coco = COCO(self.imgdir+'\\\\'+cocofilename)\n",
    "        self.list = list(self.coco.imgs.keys())\n",
    "        self.resizer = None\n",
    "        if size != None:\n",
    "            self.resizer = torchvision.transforms.v2.Resize(size,torchvision.transforms.InterpolationMode.BICUBIC,antialias=False)\n",
    "    def __len__(self):\n",
    "        return len(self.list)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.coco.imgs[idx]\n",
    "        filename = self.imgdir+'\\\\'+img['file_name']\n",
    "        image = torchvision.io.read_image(filename,torchvision.io.ImageReadMode.RGB)\n",
    "        cat_ids = self.coco.getCatIds()\n",
    "        anns_ids = self.coco.getAnnIds(imgIds=img['id'], catIds=cat_ids, iscrowd=None)\n",
    "        anns = self.coco.loadAnns(anns_ids)\n",
    "        mask = np.zeros((img['height'], img['width']), dtype=int)\n",
    "        for i in range(len(anns)):\n",
    "            mask |= self.coco.annToMask(anns[i])\n",
    "        mask = torch.from_numpy(mask)\n",
    "        if self.resizer != None:\n",
    "            image, mask = self.resizer(torchvision.tv_tensors.Image(image), torchvision.tv_tensors.Mask(mask))\n",
    "        return image, mask\n",
    "def display(display_list,save=None):\n",
    "    plt.figure(figsize=(3*5, len(display_list)*5))\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(len(display_list), 3, i*3+1)\n",
    "        plt.title('Input Image')\n",
    "        plt.imshow(display_list[i]['image'].permute(1,2,0)) #pytorch loves channels first but matplotlib loves channels last\n",
    "        plt.axis('off')\n",
    "        plt.subplot(len(display_list), 3, i*3+2)\n",
    "        plt.title('True Mask')\n",
    "        plt.imshow(display_list[i]['mask'])\n",
    "        plt.axis('off')\n",
    "        if 'pred' in display_list[i]:\n",
    "            plt.subplot(len(display_list), 3, i*3+3)\n",
    "            plt.title('Predicted Mask')\n",
    "            plt.imshow(display_list[i]['pred'])\n",
    "            plt.axis('off')\n",
    "    if save != None:\n",
    "        plt.savefig(save) #save before show\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c39a8-83f9-4f04-9fd7-b9fe542890cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72738947-401a-4a67-8868-ed570d897d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(num,channels,filters,addpool=False,reversed=False):\n",
    "    block = torch.nn.Sequential()\n",
    "    if addpool:\n",
    "        block.append(torch.nn.MaxPool2d(2, stride=2))\n",
    "    if reversed:\n",
    "        if channels == None:\n",
    "            raise NotImplementedError(\"nah son\") #Must have a channel\n",
    "        for i in range(num-1):\n",
    "            block.append(torch.nn.Conv2d(channels,channels,kernel_size=(3,3),padding='same',bias=False)) #Bias is pointless for BatchNorm\n",
    "            block.append(torch.nn.BatchNorm2d(channels))\n",
    "            block.append(torch.nn.ReLU(inplace=True))\n",
    "        block.append(torch.nn.Conv2d(channels,filters,kernel_size=(3,3),padding='same',bias=False)) #Bias is pointless for BatchNorm\n",
    "        block.append(torch.nn.BatchNorm2d(filters))\n",
    "        block.append(torch.nn.ReLU(inplace=True))\n",
    "    else:\n",
    "        for i in range(num):\n",
    "            if channels == None:\n",
    "                block.append(torch.nn.LazyConv2d(filters,kernel_size=(3,3),padding='same',bias=False)) #Bias is pointless for BatchNorm\n",
    "            else:\n",
    "                block.append(torch.nn.Conv2d(channels,filters,kernel_size=(3,3),padding='same',bias=False)) #Bias is pointless for BatchNorm\n",
    "            block.append(torch.nn.BatchNorm2d(filters))\n",
    "            block.append(torch.nn.ReLU(inplace=True))\n",
    "            channels = filters\n",
    "    return block\n",
    "class BottleResBlock(torch.nn.Module):\n",
    "    def __init__(self,channels,bottlechan,filters,stride=1,dilation=1):\n",
    "        super().__init__()\n",
    "        self.bottleneck = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(channels,bottlechan,kernel_size=(1,1),padding='valid',bias=False), #Bias is pointless for BatchNorm\n",
    "            torch.nn.BatchNorm2d(bottlechan),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(bottlechan,bottlechan,kernel_size=(3,3),padding=1+dilation-1,stride=stride,dilation=dilation,bias=False),\n",
    "            torch.nn.BatchNorm2d(bottlechan),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(bottlechan,filters,kernel_size=(1,1),padding='valid',bias=False),\n",
    "            torch.nn.BatchNorm2d(filters)\n",
    "        )\n",
    "        self.skip = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(channels,filters,kernel_size=(1,1),padding='valid',stride=stride,bias=False),\n",
    "            torch.nn.BatchNorm2d(filters),\n",
    "        ) if channels != filters or stride != 1 else torch.nn.Identity()\n",
    "    def forward(self,x):\n",
    "        x_1 = self.skip(x)\n",
    "        x_2 = self.bottleneck(x)\n",
    "        y = torch.add(x_1,x_2)\n",
    "        return y\n",
    "class InvertedBottleResBlock(torch.nn.Module):\n",
    "    def __init__(self,channels,expansion,filters,stride=1):\n",
    "        super().__init__()\n",
    "        self.bottleneck = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(channels,channels*expansion,kernel_size=(1,1),padding='valid',bias=False), #Bias is pointless for BatchNorm\n",
    "            torch.nn.BatchNorm2d(channels*expansion),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(channels*expansion,channels*expansion,kernel_size=(3,3),padding=1,stride=stride,groups=channels*expansion,bias=False), #DWConv\n",
    "            torch.nn.BatchNorm2d(channels*expansion),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(channels*expansion,filters,kernel_size=(1,1),padding='valid',bias=False),\n",
    "            torch.nn.BatchNorm2d(filters) #No ReLU here\n",
    "        )\n",
    "        self.skip = stride == 1 and channels == filters\n",
    "    def forward(self,x):\n",
    "        if self.skip:\n",
    "            x_1 = self.bottleneck(x)\n",
    "            y = torch.add(x_1,x)\n",
    "            return y\n",
    "        else:\n",
    "            y = self.bottleneck(x)\n",
    "            return y\n",
    "class PPM(torch.nn.Module):\n",
    "    def __init__(self,in_channels,pools=[1,2,4,8]):\n",
    "        super().__init__()\n",
    "        def poollayer(size):\n",
    "            block = torch.nn.Sequential()\n",
    "            if size != 1:\n",
    "                block.append(torch.nn.AvgPool2d(size, stride=size))\n",
    "            block.append(torch.nn.Conv2d(in_channels,in_channels//4,kernel_size=(1,1),bias=False))\n",
    "            block.append(torch.nn.BatchNorm2d(in_channels//4))\n",
    "            block.append(torch.nn.ReLU())\n",
    "            if size != 1:\n",
    "                block.append(torch.nn.Upsample(scale_factor=size, mode='bilinear'))\n",
    "            return block\n",
    "        self.pools = torch.nn.ModuleList([poollayer(x) for x in pools])\n",
    "        \n",
    "        #self.pool1 = poollayer(pools[0])\n",
    "        #self.pool2 = poollayer(pools[1])\n",
    "        #self.pool3 = poollayer(pools[2])\n",
    "        #self.pool4 = poollayer(pools[3])\n",
    "    def forward(self, x):\n",
    "        x_pooled = [pool(x) for pool in self.pools]\n",
    "        x_pooled.append(x)\n",
    "        y = torch.cat(x_pooled,1)\n",
    "        #x1 = self.pool1(x)\n",
    "        #x2 = self.pool2(x)\n",
    "        #x3 = self.pool3(x)\n",
    "        #x4 = self.pool4(x)\n",
    "        \n",
    "        #y = torch.cat((x,x1,x2,x3,x4),1)\n",
    "        return y\n",
    "class ASPP(torch.nn.Module):\n",
    "    def __init__(self,in_channels,out_channel_per_branch,dilations=[6,12,18]):\n",
    "        super().__init__()\n",
    "        def dilatedlayer(rate):\n",
    "            block = torch.nn.Sequential()\n",
    "            #PADDING HAS TO BE LIKE THIS\n",
    "            block.append(torch.nn.Conv2d(in_channels,out_channel_per_branch,kernel_size=(3,3),padding=rate,dilation=rate,bias=False))\n",
    "            block.append(torch.nn.BatchNorm2d(out_channel_per_branch))\n",
    "            block.append(torch.nn.ReLU())\n",
    "            return block\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels,out_channel_per_branch,kernel_size=(1,1),bias=False),\n",
    "            torch.nn.BatchNorm2d(out_channel_per_branch),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.dilations = torch.nn.ModuleList([dilatedlayer(x) for x in dilations])\n",
    "        self.globalpool = torch.nn.Sequential(\n",
    "            torch.nn.AdaptiveAvgPool2d(1),\n",
    "            torch.nn.Conv2d(in_channels,out_channel_per_branch,kernel_size=(1,1),bias=False),\n",
    "            torch.nn.BatchNorm2d(out_channel_per_branch),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.finalconv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(out_channel_per_branch*(2+len(self.dilations)),out_channel_per_branch,kernel_size=(1,1),bias=False),\n",
    "            torch.nn.BatchNorm2d(out_channel_per_branch),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        input_spatial_dim = x.size()[2:]\n",
    "        x_pooled = [pool(x) for pool in self.dilations]\n",
    "        x_pooled.append(self.conv1(x))\n",
    "        x_pooled.append(torch.nn.functional.interpolate(self.globalpool(x), input_spatial_dim, mode='bilinear', align_corners=True))\n",
    "        y = self.finalconv(torch.cat(x_pooled,1))\n",
    "        return y\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self,in_channels,classes,layers=[3,4,6,3]):\n",
    "        super().__init__()\n",
    "        def ResNetBlock(num,channels,bottlechan,filters,stride=1,addpool=False):\n",
    "            blocks = torch.nn.Sequential()\n",
    "            if addpool:\n",
    "                blocks.append(torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "            for i in range(num):\n",
    "                if stride != 1 and i == 0:\n",
    "                    blocks.append(BottleResBlock(channels,bottlechan,filters,stride))\n",
    "                else:\n",
    "                    blocks.append(BottleResBlock(channels,bottlechan,filters))\n",
    "                channels = filters\n",
    "            return blocks\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels,64,kernel_size=(7,7),stride=2,padding=3,bias=False),\n",
    "                torch.nn.BatchNorm2d(64),\n",
    "                torch.nn.ReLU(),\n",
    "            ),\n",
    "            ResNetBlock(layers[0],64,64,256,addpool=True),\n",
    "            ResNetBlock(layers[1],256,128,512,stride=2),\n",
    "            ResNetBlock(layers[2],512,256,1024,stride=2),\n",
    "            ResNetBlock(layers[3],1024,512,2048,stride=2),\n",
    "        ])\n",
    "        self.pooling = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = torch.nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.classifier = torch.nn.Linear(2048,classes)\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.blocks)):\n",
    "            x = self.blocks[i](x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.flatten(x)\n",
    "        y = self.classifier(x)\n",
    "        return y\n",
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self,in_channels,classes,convblocks=None,channels=[64,128,256,512,1024],upsample_output=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        if type(convblocks) is torch.nn.ModuleList:\n",
    "            self.encoderblocks = torch.nn.ModuleList([\n",
    "                convblocks[0],\n",
    "                convblocks[1],\n",
    "                convblocks[2],\n",
    "                convblocks[3],\n",
    "                convblocks[4]\n",
    "            ])\n",
    "            self.upsamplers = torch.nn.ModuleList([\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.LazyConvTranspose2d(self.channels[3],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                ),\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(self.channels[3],self.channels[2],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                ),\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(self.channels[2],self.channels[1],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                ),\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(self.channels[1],self.channels[0],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                )\n",
    "            ])\n",
    "            self.decoderblocks = torch.nn.ModuleList([\n",
    "                UNet._block(None,self.channels[3]),\n",
    "                UNet._block(None,self.channels[2]),\n",
    "                UNet._block(None,self.channels[1]),\n",
    "                UNet._block(None,self.channels[0])\n",
    "            ])\n",
    "        else:\n",
    "            self.encoderblocks = torch.nn.ModuleList([\n",
    "                UNet._block(in_channels,self.channels[0]),\n",
    "                UNet._block(self.channels[0],self.channels[1],True),\n",
    "                UNet._block(self.channels[1],self.channels[2],True),\n",
    "                UNet._block(self.channels[2],self.channels[3],True),\n",
    "                UNet._block(self.channels[3],self.channels[4],True)\n",
    "            ])\n",
    "            self.upsamplers = torch.nn.ModuleList([\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(self.channels[4],self.channels[3],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                ),\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(self.channels[3],self.channels[2],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                ),\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(self.channels[2],self.channels[1],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                ),\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(self.channels[1],self.channels[0],kernel_size=(2,2),stride=(2,2)),\n",
    "                    torch.nn.ReLU()\n",
    "                )\n",
    "            ])\n",
    "            self.decoderblocks = torch.nn.ModuleList([\n",
    "                UNet._block(self.channels[4],self.channels[3]),\n",
    "                UNet._block(self.channels[3],self.channels[2]),\n",
    "                UNet._block(self.channels[2],self.channels[1]),\n",
    "                UNet._block(self.channels[1],self.channels[0])\n",
    "            ])\n",
    "        self.classifier = torch.nn.Conv2d(self.channels[0],classes,kernel_size=(1,1))\n",
    "        self.upsampler = torch.nn.Upsample(scale_factor=upsample_output, mode='bilinear') if upsample_output != 1 else torch.nn.Identity()\n",
    "    @staticmethod\n",
    "    def _block(channels,filters,pool=False):\n",
    "        block = torch.nn.Sequential()\n",
    "        if pool:\n",
    "            block.append(torch.nn.MaxPool2d(2, stride=2))\n",
    "        if channels == None:\n",
    "            block.append(torch.nn.LazyConv2d(filters,kernel_size=(3,3),padding=(1,1),bias=False)) #Bias is pointless for BatchNorm\n",
    "        else:\n",
    "            block.append(torch.nn.Conv2d(channels,filters,kernel_size=(3,3),padding=(1,1),bias=False)) #Bias is pointless for BatchNorm\n",
    "        block.append(torch.nn.BatchNorm2d(filters))\n",
    "        block.append(torch.nn.ReLU(inplace=True))\n",
    "        block.append(torch.nn.Conv2d(filters,filters,kernel_size=(3,3),padding=(1,1),bias=False)) #Bias is pointless for BatchNorm\n",
    "        block.append(torch.nn.BatchNorm2d(filters))\n",
    "        block.append(torch.nn.ReLU(inplace=True))\n",
    "        return block\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoderblocks[0](x)\n",
    "        x2 = self.encoderblocks[1](x1)\n",
    "        x3 = self.encoderblocks[2](x2)\n",
    "        x4 = self.encoderblocks[3](x3)\n",
    "        x5 = self.encoderblocks[4](x4)\n",
    "        x4_2 = self.upsamplers[0](x5)\n",
    "        x4_2 = torch.cat((x4,x4_2),1)\n",
    "        x4_2 = self.decoderblocks[0](x4_2)\n",
    "        x3_2 = self.upsamplers[1](x4_2)\n",
    "        x3_2 = torch.cat((x3,x3_2),1)\n",
    "        x3_2 = self.decoderblocks[1](x3_2)\n",
    "        x2_2 = self.upsamplers[2](x3_2)\n",
    "        x2_2 = torch.cat((x2,x2_2),1)\n",
    "        x2_2 = self.decoderblocks[2](x2_2)\n",
    "        x1_2 = self.upsamplers[3](x2_2)\n",
    "        x1_2 = torch.cat((x1,x1_2),1)\n",
    "        x1_2 = self.decoderblocks[3](x1_2)\n",
    "        y = self.classifier(x1_2)\n",
    "        y = self.upsampler(y)\n",
    "        return y\n",
    "class UNetPPM(UNet):\n",
    "    def __init__(self,in_channels,classes,convblocks=None,encoderblocks=[2,2,2,2,2],decoderblocks=[2,2,2,2],upsample_output=1,pools=[1,2,4,8]):\n",
    "        super().__init__(in_channels,classes,convblocks,encoderblocks,decoderblocks,upsample_output)\n",
    "        self.decoderblocks[3] = torch.nn.Sequential(\n",
    "            PPM(self.channels[1],pools),\n",
    "            ConvBlock(decoderblocks[3],(self.channels[1]//4)*len(pools)+self.channels[1],self.channels[0]),\n",
    "        )\n",
    "        #self.classifier = torch.nn.Conv2d(self.channels[0],CLASSES,kernel_size=(1,1),padding='same')\n",
    "class UNetASPP(UNet):\n",
    "    def __init__(self,in_channels,classes,convblocks=None,encoderblocks=[2,2,2,2,2],decoderblocks=[2,2,2,2],upsample_output=1,dilations=[6,12,18]):\n",
    "        super().__init__(in_channels,classes,convblocks,encoderblocks,decoderblocks,upsample_output)\n",
    "        self.decoderblocks[3] = torch.nn.Sequential(\n",
    "            #ASPP(self.channels[1],self.channels[0],dilations),\n",
    "            ConvBlock(decoderblocks[3],self.channels[1],self.channels[0]),\n",
    "            ASPP(self.channels[0],self.channels[0],dilations),\n",
    "        )\n",
    "class SegNet(torch.nn.Module):\n",
    "    def __init__(self,in_channels,classes,convblocks=None,ptdec=False):\n",
    "        super().__init__()\n",
    "        if type(convblocks) is torch.nn.ModuleList:\n",
    "            self.encoderblocks = torch.nn.ModuleList([\n",
    "                convblocks[0],\n",
    "                convblocks[1],\n",
    "                convblocks[2],\n",
    "                convblocks[3],\n",
    "                convblocks[4]\n",
    "            ])\n",
    "        else:\n",
    "            backbone = torch.hub.load('pytorch/vision', 'vgg16_bn', weights='None')\n",
    "            self.encoderblocks = torch.nn.ModuleList([\n",
    "                SegNet._seq(backbone.features[0:6]),\n",
    "                SegNet._seq(backbone.features[7:13]),\n",
    "                SegNet._seq(backbone.features[14:23]),\n",
    "                SegNet._seq(backbone.features[24:33]),\n",
    "                SegNet._seq(backbone.features[34:43]),\n",
    "            ])\n",
    "            del backbone\n",
    "        self.downsamplers = torch.nn.ModuleList([\n",
    "            torch.nn.MaxPool2d(2, stride=2,return_indices=True),\n",
    "            torch.nn.MaxPool2d(2, stride=2,return_indices=True),\n",
    "            torch.nn.MaxPool2d(2, stride=2,return_indices=True),\n",
    "            torch.nn.MaxPool2d(2, stride=2,return_indices=True),\n",
    "            torch.nn.MaxPool2d(2, stride=2,return_indices=True)\n",
    "        ])\n",
    "        self.upsamplers = torch.nn.ModuleList([\n",
    "            torch.nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.MaxUnpool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        ])\n",
    "        if ptdec:\n",
    "            backbone = torch.hub.load('pytorch/vision', 'vgg16_bn', weights='VGG16_BN_Weights.IMAGENET1K_V1')\n",
    "        else:\n",
    "            backbone = torch.hub.load('pytorch/vision', 'vgg16_bn', weights=None)\n",
    "        self.decoderblocks = torch.nn.ModuleList([\n",
    "            SegNet._seq(backbone.features[34:43]),\n",
    "            SegNet._seq(backbone.features[27:33]), #24:33\n",
    "            SegNet._seq(backbone.features[17:23]), #14:23\n",
    "            SegNet._seq(backbone.features[10:13]), #7:13\n",
    "            SegNet._seq(backbone.features[3:6]), #0:6\n",
    "        ])\n",
    "        self.decoderblocks[1].append(torch.nn.Conv2d(512,256,kernel_size=(3,3),padding=(1,1),bias=False)) #Bias is pointless for BatchNorm\n",
    "        self.decoderblocks[1].append(torch.nn.BatchNorm2d(256))\n",
    "        self.decoderblocks[1].append(torch.nn.ReLU(inplace=True))\n",
    "        self.decoderblocks[2].append(torch.nn.Conv2d(256,128,kernel_size=(3,3),padding=(1,1),bias=False)) #Bias is pointless for BatchNorm\n",
    "        self.decoderblocks[2].append(torch.nn.BatchNorm2d(128))\n",
    "        self.decoderblocks[2].append(torch.nn.ReLU(inplace=True))\n",
    "        self.decoderblocks[3].append(torch.nn.Conv2d(128,64,kernel_size=(3,3),padding=(1,1),bias=False)) #Bias is pointless for BatchNorm\n",
    "        self.decoderblocks[3].append(torch.nn.BatchNorm2d(64))\n",
    "        self.decoderblocks[3].append(torch.nn.ReLU(inplace=True))\n",
    "        self.decoderblocks[4].append(torch.nn.Conv2d(64,64,kernel_size=(3,3),padding=(1,1),bias=False)) #Bias is pointless for BatchNorm\n",
    "        self.decoderblocks[4].append(torch.nn.BatchNorm2d(64))\n",
    "        self.decoderblocks[4].append(torch.nn.ReLU(inplace=True))\n",
    "        self.decoderblocks[4].append(torch.nn.Conv2d(64,classes,kernel_size=(1,1))) #Classifier\n",
    "        del backbone\n",
    "    @staticmethod\n",
    "    def _seq(blocks):\n",
    "        newblocks = torch.nn.Sequential()\n",
    "        for i in blocks:\n",
    "            newblocks.append(i)\n",
    "        return newblocks\n",
    "    def forward(self, x):\n",
    "        x = self.encoderblocks[0](x)\n",
    "        x, i1 = self.downsamplers[0](x)\n",
    "        x = self.encoderblocks[1](x)\n",
    "        x, i2 = self.downsamplers[0](x)\n",
    "        x = self.encoderblocks[2](x)\n",
    "        x, i3 = self.downsamplers[0](x)\n",
    "        x = self.encoderblocks[3](x)\n",
    "        x, i4 = self.downsamplers[0](x)\n",
    "        x = self.encoderblocks[4](x)\n",
    "        x, i5 = self.downsamplers[0](x)\n",
    "        x = self.upsamplers[0](x, indices=i5)\n",
    "        x = self.decoderblocks[0](x)\n",
    "        x = self.upsamplers[1](x, indices=i4)\n",
    "        x = self.decoderblocks[1](x)\n",
    "        x = self.upsamplers[2](x, indices=i3)\n",
    "        x = self.decoderblocks[2](x)\n",
    "        x = self.upsamplers[3](x, indices=i2)\n",
    "        x = self.decoderblocks[3](x)\n",
    "        x = self.upsamplers[4](x, indices=i1)\n",
    "        x = self.decoderblocks[4](x)\n",
    "        return x\n",
    "class FCNBase(torch.nn.Module):\n",
    "    def __init__(self,in_channels,classes,convblocks,upsamplevar=8):\n",
    "        super().__init__()\n",
    "        if not type(convblocks) is torch.nn.ModuleList:\n",
    "            assert(False)\n",
    "        if upsamplevar != 32 and upsamplevar != 16 and upsamplevar != 8:\n",
    "            assert(False)\n",
    "        self.fcnvar=upsamplevar\n",
    "        self.encoderblocks = torch.nn.ModuleList([\n",
    "            convblocks[0],\n",
    "            convblocks[1],\n",
    "            convblocks[2],\n",
    "            convblocks[3],\n",
    "            convblocks[4]\n",
    "        ])\n",
    "        #NOTE: I think padding in ConvTranspose2d works similar to centercrop\n",
    "        #self.upsampler4 = torch.nn.ConvTranspose2d(classes,classes,kernel_size=(4,4),stride=(2,2),padding=(1,1),bias=False) #4 Kernel size, as intended\n",
    "        #self.upsampler5 = torch.nn.ConvTranspose2d(classes,classes,kernel_size=(4,4),stride=(2,2),padding=(1,1),bias=False) #in the paper's source code\n",
    "        self.upsampler4 = torch.nn.Upsample(scale_factor=2, mode='bilinear') #The original code uses ConvTranspose2d but with frozen params\n",
    "        self.upsampler5 = torch.nn.Upsample(scale_factor=2, mode='bilinear') #Therefore, upsampling is replicated with bilinear upsampling\n",
    "        self.classifier3 = torch.nn.LazyConv2d(classes,kernel_size=(1,1))\n",
    "        self.classifier4 = torch.nn.LazyConv2d(classes,kernel_size=(1,1))\n",
    "        self.classifier5 = torch.nn.LazyConv2d(classes,kernel_size=(1,1))\n",
    "        #self.finalupsampler = torch.nn.ConvTranspose2d(classes,classes,kernel_size=(self.fcnvar*2,self.fcnvar*2),\n",
    "        #                                               stride=(self.fcnvar,self.fcnvar),padding=(self.fcnvar//2,self.fcnvar//2),bias=False)\n",
    "        self.finalupsampler = torch.nn.Upsample(scale_factor=upsamplevar, mode='bilinear')\n",
    "    def forward(self, x0):\n",
    "        x1 = self.encoderblocks[0](x0) #x1 = /2\n",
    "        x2 = self.encoderblocks[1](x1) #x2 = /4\n",
    "        x3 = self.encoderblocks[2](x2) #x3 = /8\n",
    "        x4 = self.encoderblocks[3](x3) #x4 = /16\n",
    "        x5 = self.encoderblocks[4](x4) #x5 = /32\n",
    "        p5 = self.classifier5(x5)\n",
    "        if self.fcnvar == 32:\n",
    "            return self.finalupsampler(p5)\n",
    "        p4 = self.classifier4(x4*0.01) #0.01 = Scaling as intended in the paper\n",
    "        p5 = self.upsampler5(p5)\n",
    "        p4 = torch.add(p4,p5)\n",
    "        if self.fcnvar == 16:\n",
    "            return self.finalupsampler(p4)\n",
    "        p3 = self.classifier3(x3*0.0001) #0.0001 = Scaling as intended in the paper\n",
    "        p4 = self.upsampler4(p4)\n",
    "        p3 = torch.add(p3,p4)\n",
    "        return self.finalupsampler(p3)\n",
    "    def visualizeout(self, x0):\n",
    "        x1 = self.encoderblocks[0](x0) #x1 = /2\n",
    "        x2 = self.encoderblocks[1](x1) #x2 = /4\n",
    "        x3 = self.encoderblocks[2](x2) #x3 = /8\n",
    "        x4 = self.encoderblocks[3](x3) #x4 = /16\n",
    "        x5 = self.encoderblocks[4](x4) #x5 = /32\n",
    "        p5 = self.classifier5(x5)\n",
    "        p4 = self.classifier4(x4*0.01)\n",
    "        p3 = self.classifier3(x3*0.0001)\n",
    "        return (p5,p4,p3)\n",
    "def INF(B,H,W):\n",
    "    return -torch.diag(torch.tensor(float(\"inf\")).cuda().repeat(H),0).unsqueeze(0).repeat(B*W,1,1)\n",
    "class CrissCrossAttention(torch.nn.Module):\n",
    "    \"\"\" Criss-Cross Attention Module, copied straight from the repo\"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(CrissCrossAttention,self).__init__()\n",
    "        \n",
    "        self.query_conv = torch.nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.key_conv = torch.nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.value_conv = torch.nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.softmax = torch.nn.Softmax(dim=3)\n",
    "        self.INF = INF\n",
    "        self.gamma = torch.nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x):\n",
    "        m_batchsize, _, height, width = x.size()\n",
    "        proj_query = self.query_conv(x)\n",
    "        proj_query_H = proj_query.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height).permute(0, 2, 1)\n",
    "        proj_query_W = proj_query.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x)\n",
    "        proj_key_H = proj_key.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height)\n",
    "        proj_key_W = proj_key.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width)\n",
    "        proj_value = self.value_conv(x)\n",
    "        proj_value_H = proj_value.permute(0,3,1,2).contiguous().view(m_batchsize*width,-1,height)\n",
    "        proj_value_W = proj_value.permute(0,2,1,3).contiguous().view(m_batchsize*height,-1,width)\n",
    "        energy_H = (torch.bmm(proj_query_H, proj_key_H)+self.INF(m_batchsize, height, width)).view(m_batchsize,width,height,height).permute(0,2,1,3)\n",
    "        energy_W = torch.bmm(proj_query_W, proj_key_W).view(m_batchsize,height,width,width)\n",
    "        concate = self.softmax(torch.cat([energy_H, energy_W], 3))\n",
    "        att_H = concate[:,:,:,0:height].permute(0,2,1,3).contiguous().view(m_batchsize*width,height,height)\n",
    "        #print(concate)\n",
    "        #print(att_H)\n",
    "        att_W = concate[:,:,:,height:height+width].contiguous().view(m_batchsize*height,width,width)\n",
    "        out_H = torch.bmm(proj_value_H, att_H.permute(0, 2, 1)).view(m_batchsize,width,-1,height).permute(0,2,3,1)\n",
    "        out_W = torch.bmm(proj_value_W, att_W.permute(0, 2, 1)).view(m_batchsize,height,-1,width).permute(0,2,1,3)\n",
    "        #print(out_H.size(),out_W.size())\n",
    "        return self.gamma*(out_H + out_W) + x\n",
    "class RCCAModule(torch.nn.Module):\n",
    "    \"\"\"Also copied too\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, num_classes):\n",
    "        super(RCCAModule, self).__init__()\n",
    "        inter_channels = in_channels // 4\n",
    "        self.conva = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(inter_channels),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.cca = CrissCrossAttention(inter_channels)\n",
    "        self.convb = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(inter_channels),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.bottleneck = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels+inter_channels, out_channels, kernel_size=3, padding=1, dilation=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout2d(0.1),\n",
    "            torch.nn.Conv2d(512, num_classes, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        )\n",
    "    def forward(self, x, recurrence=1):\n",
    "        output = self.conva(x)\n",
    "        for i in range(recurrence):\n",
    "            output = self.cca(output)\n",
    "        output = self.convb(output)\n",
    "        output = self.bottleneck(torch.cat([x, output], 1))\n",
    "        return output\n",
    "class DilatedResNet(torch.nn.Module):\n",
    "    def __init__(self,in_channels,classes,layers=[3,4,6,3]):\n",
    "        super().__init__()\n",
    "        def ResNetBlock(num,channels,bottlechan,filters,stride=1,dilation=1,addpool=False):\n",
    "            blocks = torch.nn.Sequential()\n",
    "            if addpool:\n",
    "                blocks.append(torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "            for i in range(num):\n",
    "                if stride != 1 and i == 0:\n",
    "                    blocks.append(BottleResBlock(channels,bottlechan,filters,stride,dilation=dilation))\n",
    "                else:\n",
    "                    blocks.append(BottleResBlock(channels,bottlechan,filters,dilation=dilation))\n",
    "                channels = filters\n",
    "            return blocks\n",
    "        self.blocks = torch.nn.Sequential(\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels,64,kernel_size=(7,7),stride=2,padding=3,bias=False),\n",
    "                torch.nn.BatchNorm2d(64),\n",
    "                torch.nn.ReLU(),\n",
    "            ),\n",
    "            ResNetBlock(layers[0],64,64,256,addpool=True),\n",
    "            ResNetBlock(layers[1],256,128,512,stride=2),\n",
    "            ResNetBlock(layers[2],512,256,1024,stride=1,dilation=2),\n",
    "            ResNetBlock(layers[3],1024,512,2048,stride=1,dilation=4),\n",
    "        )\n",
    "        self.head = torch.nn.Conv2d(2048,classes,kernel_size=(1,1),padding='same')\n",
    "        self.upsampler = torch.nn.Upsample(scale_factor=8, mode='bilinear')\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        y = self.head(x)\n",
    "        y = self.upsampler(y)\n",
    "        return y\n",
    "class CCNet(DilatedResNet):\n",
    "    def __init__(self,in_channels,classes,layers=[3,4,6,3],recurrence=2):\n",
    "        super().__init__(in_channels,classes,layers=[3,4,6,3])\n",
    "        self.head = RCCAModule(2048, 512, classes)\n",
    "        self.upsampler = torch.nn.Upsample(scale_factor=8, mode='bilinear')\n",
    "        self.recur = recurrence\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        y = self.head(x,self.recur)\n",
    "        y = self.upsampler(y)\n",
    "        return y\n",
    "class DilatedResNetWithPPM(DilatedResNet):\n",
    "    def __init__(self,in_channels,classes,layers=[3,4,6,3]):\n",
    "        super().__init__(in_channels,classes,layers=[3,4,6,3])\n",
    "        self.ppm = PPM(2048)\n",
    "        self.head = torch.nn.Conv2d((2048//4)*4+2048,classes,kernel_size=(1,1),padding='same')\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        x = self.ppm(x)\n",
    "        y = self.head(x)\n",
    "        y = self.upsampler(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00a7f41-57df-4330-b1d1-406972d18f2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3007d4-614d-4052-9faa-3230ced46446",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossentropyND(torch.nn.CrossEntropyLoss):\n",
    "    \"\"\"\n",
    "    Network has to have NO NONLINEARITY!\n",
    "    \"\"\"\n",
    "    def forward(self, inp, target):\n",
    "        target = target.long()\n",
    "        num_classes = inp.size()[1]\n",
    "\n",
    "        i0 = 1\n",
    "        i1 = 2\n",
    "\n",
    "        while i1 < len(inp.shape): # this is ugly but torch only allows to transpose two axes at once\n",
    "            inp = inp.transpose(i0, i1)\n",
    "            i0 += 1\n",
    "            i1 += 1\n",
    "\n",
    "        inp = inp.contiguous()\n",
    "        inp = inp.view(-1, num_classes)\n",
    "\n",
    "        target = target.contiguous()\n",
    "        target = target.view(-1,)\n",
    "\n",
    "        return super(CrossentropyND, self).forward(inp, target)\n",
    "\n",
    "class TopKLoss(CrossentropyND):\n",
    "    \"\"\"\n",
    "    Network has to have NO LINEARITY!\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, ignore_index=-100, k=10):\n",
    "        self.k = k\n",
    "        super(TopKLoss, self).__init__(weight, False, ignore_index, reduce=False)\n",
    "\n",
    "    def forward(self, inp, target):\n",
    "        #target = target[:, 0].long()\n",
    "        res = super(TopKLoss, self).forward(inp, target)\n",
    "        num_voxels = np.prod(res.shape)\n",
    "        res, _ = torch.topk(res.view((-1, )), int(num_voxels * self.k / 100), sorted=False)\n",
    "        return res.mean()\n",
    "\n",
    "def sum_tensor(inp, axes, keepdim=False):\n",
    "    # copy from: https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/utilities/tensor_utilities.py\n",
    "    axes = np.unique(axes).astype(int)\n",
    "    if keepdim:\n",
    "        for ax in axes:\n",
    "            inp = inp.sum(int(ax), keepdim=True)\n",
    "    else:\n",
    "        for ax in sorted(axes, reverse=True):\n",
    "            inp = inp.sum(int(ax))\n",
    "    return inp\n",
    "    \n",
    "def get_tp_fp_fn(net_output, gt, axes=None, mask=None, square=False):\n",
    "    \"\"\"\n",
    "    net_output must be (b, c, x, y(, z)))\n",
    "    gt must be a label map (shape (b, 1, x, y(, z)) OR shape (b, x, y(, z))) or one hot encoding (b, c, x, y(, z))\n",
    "    if mask is provided it must have shape (b, 1, x, y(, z)))\n",
    "    :param net_output:\n",
    "    :param gt:\n",
    "    :param axes:\n",
    "    :param mask: mask must be 1 for valid pixels and 0 for invalid pixels\n",
    "    :param square: if True then fp, tp and fn will be squared before summation\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        axes = tuple(range(2, len(net_output.size())))\n",
    "\n",
    "    shp_x = net_output.shape\n",
    "    shp_y = gt.shape\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if len(shp_x) != len(shp_y):\n",
    "            gt = gt.view((shp_y[0], 1, *shp_y[1:]))\n",
    "\n",
    "        if all([i == j for i, j in zip(net_output.shape, gt.shape)]):\n",
    "            # if this is the case then gt is probably already a one hot encoding\n",
    "            y_onehot = gt\n",
    "        else:\n",
    "            gt = gt.long()\n",
    "            y_onehot = torch.zeros(shp_x)\n",
    "            if net_output.device.type == \"cuda\":\n",
    "                y_onehot = y_onehot.cuda(net_output.device.index)\n",
    "            y_onehot.scatter_(1, gt, 1)\n",
    "\n",
    "    tp = net_output * y_onehot\n",
    "    fp = net_output * (1 - y_onehot)\n",
    "    fn = (1 - net_output) * y_onehot\n",
    "\n",
    "    if mask is not None:\n",
    "        tp = torch.stack(tuple(x_i * mask[:, 0] for x_i in torch.unbind(tp, dim=1)), dim=1)\n",
    "        fp = torch.stack(tuple(x_i * mask[:, 0] for x_i in torch.unbind(fp, dim=1)), dim=1)\n",
    "        fn = torch.stack(tuple(x_i * mask[:, 0] for x_i in torch.unbind(fn, dim=1)), dim=1)\n",
    "\n",
    "    if square:\n",
    "        tp = tp ** 2\n",
    "        fp = fp ** 2\n",
    "        fn = fn ** 2\n",
    "\n",
    "    tp = sum_tensor(tp, axes, keepdim=False)\n",
    "    fp = sum_tensor(fp, axes, keepdim=False)\n",
    "    fn = sum_tensor(fn, axes, keepdim=False)\n",
    "\n",
    "    return tp, fp, fn\n",
    "    \n",
    "class SoftDiceLoss(torch.nn.Module):\n",
    "    def __init__(self, apply_nonlin=None, batch_dice=False, do_bg=True, smooth=1.,\n",
    "                 square=False):\n",
    "        \"\"\"\n",
    "        paper: https://arxiv.org/pdf/1606.04797.pdf\n",
    "        \"\"\"\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "\n",
    "        self.square = square\n",
    "        self.do_bg = do_bg\n",
    "        self.batch_dice = batch_dice\n",
    "        self.apply_nonlin = apply_nonlin\n",
    "        self.smooth = smooth\n",
    "    def forward(self, x, y, loss_mask=None):\n",
    "        shp_x = x.shape\n",
    "\n",
    "        if self.batch_dice:\n",
    "            axes = [0] + list(range(2, len(shp_x)))\n",
    "        else:\n",
    "            axes = list(range(2, len(shp_x)))\n",
    "\n",
    "        if self.apply_nonlin is not None:\n",
    "            x = self.apply_nonlin(x)\n",
    "\n",
    "        tp, fp, fn = get_tp_fp_fn(x, y, axes, loss_mask, self.square)\n",
    "\n",
    "        dc = (2 * tp + self.smooth) / (2 * tp + fp + fn + self.smooth)\n",
    "\n",
    "        if not self.do_bg:\n",
    "            if self.batch_dice:\n",
    "                dc = dc[1:]\n",
    "            else:\n",
    "                dc = dc[:, 1:]\n",
    "        dc = dc.mean()\n",
    "\n",
    "        return -dc\n",
    "        \n",
    "class DC_and_CE_loss(torch.nn.Module):\n",
    "    def __init__(self, apply_nonlin=None, aggregate=\"sum\"):\n",
    "        super(DC_and_CE_loss, self).__init__()\n",
    "        self.aggregate = aggregate\n",
    "        self.ce = CrossentropyND()\n",
    "        self.dc = SoftDiceLoss(apply_nonlin)\n",
    "    def forward(self, net_output, target):\n",
    "        dc_loss = self.dc(net_output, target)\n",
    "        ce_loss = self.ce(net_output, target)\n",
    "        if self.aggregate == \"sum\":\n",
    "            result = ce_loss + dc_loss\n",
    "        else:\n",
    "            raise NotImplementedError(\"nah son\") # reserved for other stuff (later)\n",
    "        return result\n",
    "\n",
    "class TverskyLoss(torch.nn.Module):\n",
    "    def __init__(self, apply_nonlin=None, alpha=0.3, beta=0.7, batch_dice=False, do_bg=True, smooth=1.,\n",
    "                 square=False):\n",
    "        \"\"\"\n",
    "        paper: https://arxiv.org/pdf/1706.05721.pdf\n",
    "        \"\"\"\n",
    "        super(TverskyLoss, self).__init__()\n",
    "\n",
    "        self.square = square\n",
    "        self.do_bg = do_bg\n",
    "        self.batch_dice = batch_dice\n",
    "        self.apply_nonlin = apply_nonlin\n",
    "        self.smooth = smooth\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, x, y, loss_mask=None):\n",
    "        shp_x = x.shape\n",
    "\n",
    "        if self.batch_dice:\n",
    "            axes = [0] + list(range(2, len(shp_x)))\n",
    "        else:\n",
    "            axes = list(range(2, len(shp_x)))\n",
    "\n",
    "        if self.apply_nonlin is not None:\n",
    "            x = self.apply_nonlin(x)\n",
    "\n",
    "        tp, fp, fn = get_tp_fp_fn(x, y, axes, loss_mask, self.square)\n",
    "\n",
    "\n",
    "        tversky = (tp + self.smooth) / (tp + self.alpha*fp + self.beta*fn + self.smooth)\n",
    "\n",
    "        if not self.do_bg:\n",
    "            if self.batch_dice:\n",
    "                tversky = tversky[1:]\n",
    "            else:\n",
    "                tversky = tversky[:, 1:]\n",
    "        tversky = tversky.mean()\n",
    "\n",
    "        return -tversky\n",
    "\n",
    "class FocalTversky_loss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    paper: https://arxiv.org/pdf/1810.07842.pdf\n",
    "    author code: https://github.com/nabsabraham/focal-tversky-unet/blob/347d39117c24540400dfe80d106d2fb06d2b99e1/losses.py#L65\n",
    "    \"\"\"\n",
    "    def __init__(self, tversky_kwargs, gamma=0.75):\n",
    "        super(FocalTversky_loss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.tversky = TverskyLoss(**tversky_kwargs)\n",
    "\n",
    "    def forward(self, net_output, target):\n",
    "        tversky_loss = 1 + self.tversky(net_output, target) # = 1-tversky(net_output, target)\n",
    "        focal_tversky = torch.pow(tversky_loss, self.gamma)\n",
    "        return focal_tversky"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4db63-b0d7-4fbf-903a-1e5e946114aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52747a-f0ed-40be-afb7-2a5f7ab84e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(y,classes):\n",
    "    return torch.nn.functional.one_hot(y.long(),classes).permute(0,3,1,2).float() #cursed\n",
    "def iou(pred, target, n_classes = 2, include_bg_class = True):\n",
    "    ious = []\n",
    "    # Assuming the shapes are BATCH x 1 x H x W => BATCH x H x W\n",
    "    pred = pred.cpu()\n",
    "    target = target.cpu()\n",
    "    for cls in range(0 if include_bg_class else 1, n_classes):  # This goes from 1:n_classes-1 -> class \"0\" is ignored\n",
    "        pred2 = pred == cls\n",
    "        target2 = target == cls\n",
    "        intersection = torch.logical_and(pred2,target2).count_nonzero((1,2)).data.cpu().numpy()\n",
    "        #union = torch.logical_or(pred2,target2).count_nonzero((1,2)).data.cpu().item()\n",
    "        union = pred2.count_nonzero((1,2)).data.cpu().numpy() + target2.count_nonzero((1,2)).data.cpu().numpy() - intersection\n",
    "        ious.append((intersection/union))\n",
    "    return ious\n",
    "def fone(pred, target, n_classes = 2, include_bg_class = True): #a.k.a. Dice\n",
    "    fone = []\n",
    "    # Assuming the shapes are BATCH x 1 x H x W => BATCH x H x W\n",
    "    pred = pred.cpu()\n",
    "    target = target.cpu()\n",
    "    for cls in range(0 if include_bg_class else 1, n_classes):  # This goes from 1:n_classes-1 -> class \"0\" is ignored\n",
    "        pred2 = pred == cls\n",
    "        target2 = target == cls\n",
    "        intersectcount = torch.logical_and(pred2,target2).count_nonzero((1,2)).data.cpu().numpy()\n",
    "        targetcount = target2.count_nonzero((1,2)).data.cpu().numpy()\n",
    "        predcount = pred2.count_nonzero((1,2)).data.cpu().numpy()\n",
    "        fone.append(2*intersectcount/(targetcount+predcount))\n",
    "    return fone\n",
    "class EarlyStopper:\n",
    "    def __init__(self, metric_name='val_loss', lower_is_better=True, patience=4, grace_period=0):\n",
    "        self.metric_name = metric_name\n",
    "        self.patience = patience\n",
    "        self.grace_period = grace_period\n",
    "        self.grace_count = 0\n",
    "        self.counter = 0\n",
    "        self.lower_is_better = lower_is_better\n",
    "        if self.lower_is_better:\n",
    "            self.best_metric = float('inf')\n",
    "        else:\n",
    "            self.best_metric = -float('inf')\n",
    "    def updatebest(self,metric):\n",
    "        if self.lower_is_better:\n",
    "            if metric < self.best_metric:\n",
    "                self.best_metric = metric\n",
    "                return True\n",
    "        else:\n",
    "            if metric > self.best_metric:\n",
    "                self.best_metric = metric\n",
    "                return True\n",
    "        return False\n",
    "    def early_stop(self, history, lower_is_better=True):\n",
    "        metric = history[self.metric_name][-1]\n",
    "        if self.grace_count < self.grace_period:\n",
    "            self.grace_count += 1\n",
    "            return -1 if self.updatebest(metric) else 0\n",
    "            #return 0\n",
    "        if self.updatebest(metric):\n",
    "            self.counter = 0\n",
    "            return -1\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return 1\n",
    "        return 0\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        if self.lower_is_better:\n",
    "            self.best_metric = float('inf')\n",
    "        else:\n",
    "            self.best_metric = -float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808614b7-cdf1-492b-817e-2cd6a0a5530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manager():\n",
    "    def __init__(self, train_dl, val_dl, model, loss_fn, optimizer, preprocesser = None, scheduler = None, multi_losses = None):\n",
    "        self.epoch = 0\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.preprocesser = preprocesser\n",
    "        self.scheduler = scheduler\n",
    "        self.multi_losses = multi_losses\n",
    "        parametertensor = next(self.model.parameters())\n",
    "        self.to_device = parametertensor.device\n",
    "        self.rescaler = torchvision.transforms.v2.ToDtype(parametertensor.dtype,True) #infer the dtype of input\n",
    "        self.stopnow = False\n",
    "        self.personalbest = False\n",
    "        self.pbrecord = None\n",
    "        self.history = {'train_loss': [], 'train_miou': [], 'train_f1': [], 'val_loss': [], 'val_miou': [], 'val_f1': [], 'best_epoch': 0}\n",
    "    def train_one_epoch(self):\n",
    "        size = len(self.train_dl.dataset)\n",
    "        count = 0\n",
    "        batchsizes, losses, ious, fones = [], [], [], []\n",
    "        self.model.train()\n",
    "        for X, y in self.train_dl:\n",
    "            #preprocess layers here\n",
    "            X = self.rescaler(X).to(device=self.to_device,memory_format=torch.channels_last)\n",
    "            if self.preprocesser != None:\n",
    "                X, y = self.preprocesser(X, torchvision.tv_tensors.Mask(y))\n",
    "            y = y.to(device=self.to_device,dtype=torch.int64)\n",
    "            #ohe_y = ohe(y.to(device=self.to_device,dtype=torch.int64),2)\n",
    "            # Compute prediction and loss\n",
    "            out = self.model(X)\n",
    "            #\n",
    "            if type(out) is OrderedDict:\n",
    "                if \"out\" in out:\n",
    "                    loss = self.loss_fn(out[\"out\"], y)\n",
    "                    pred = out[\"out\"]\n",
    "                else:\n",
    "                    assert(False) #Model doesn't produce output!\n",
    "                if \"aux\" in out:\n",
    "                    #Check whether aux losses are defined\n",
    "                    if type(self.multi_losses) is OrderedDict and \"aux\" in self.multi_losses:\n",
    "                        #Do aux losses\n",
    "                        loss += self.loss_fn(out[\"aux\"], y)*self.multi_losses[\"aux\"]\n",
    "                    else:\n",
    "                        assert(False) #Unnecessary aux output! Disable it!\n",
    "            else:\n",
    "                #Good ol' tensor output\n",
    "                loss = self.loss_fn(out, y)\n",
    "                pred = out\n",
    "            #print(pred.shape)\n",
    "            #print(y.shape)\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            # Stop propagating gradient\n",
    "            loss = loss.item()\n",
    "            # and relevant metrics\n",
    "            pred = pred.argmax(1)\n",
    "            biou = iou(pred,y,2,False)\n",
    "            bfone = fone(pred,y,2,False)\n",
    "            #\n",
    "            batchsizes.append(len(X))\n",
    "            losses.append(loss)\n",
    "            for i in biou:\n",
    "                ious.extend(i.tolist())\n",
    "            for i in bfone:\n",
    "                fones.extend(i.tolist())\n",
    "            count = count + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{count:>5d}/{size:>5d}]\",end = '\\r')\n",
    "        print()\n",
    "        meanloss = np.average(np.array(losses),weights=np.array(batchsizes))\n",
    "        meaniou = np.ma.mean(np.ma.masked_invalid(ious))\n",
    "        meanfone = np.ma.mean(np.ma.masked_invalid(fones))\n",
    "        print(f\"Training Error:   mIoU: {(100*meaniou):>.02f}%, F1: {(100*meanfone):>.02f}%, Avg loss: {meanloss:>8f}\")\n",
    "        self.history['train_loss'].append(meanloss)\n",
    "        self.history['train_miou'].append(meaniou)\n",
    "        self.history['train_f1'].append(meanfone)\n",
    "    def validate(self, earlystop):\n",
    "        size = len(self.val_dl.dataset)\n",
    "        self.model.eval()\n",
    "        num_batches = len(self.val_dl)\n",
    "        batchsizes, losses, ious, fones = [], [], [], []\n",
    "        with torch.no_grad(): # reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "            for X, y in self.val_dl:\n",
    "                X = self.rescaler(X).to(device=self.to_device,memory_format=torch.channels_last)\n",
    "                y = y.to(device=self.to_device,dtype=torch.int64)\n",
    "                #ohe_y = ohe(y.to(device=self.to_device,dtype=torch.int64),2)\n",
    "                # Compute prediction and loss\n",
    "                out = self.model(X)\n",
    "                #\n",
    "                if type(out) is OrderedDict:\n",
    "                    if \"out\" in out:\n",
    "                        loss = self.loss_fn(out[\"out\"], y)\n",
    "                        pred = out[\"out\"]\n",
    "                    else:\n",
    "                        assert(False) #Model doesn't produce output!\n",
    "                    if \"aux\" in out:\n",
    "                        #Check whether aux losses are defined\n",
    "                        if type(self.multi_losses) is OrderedDict and \"aux\" in self.multi_losses:\n",
    "                            #Do aux losses\n",
    "                            loss += self.loss_fn(out[\"aux\"], y)*self.multi_losses[\"aux\"]\n",
    "                        else:\n",
    "                            assert(False) #Unnecessary aux output! Disable it!\n",
    "                else:\n",
    "                    #Good ol' tensor output\n",
    "                    loss = self.loss_fn(out, y)\n",
    "                    pred = out\n",
    "                loss = loss.item() #Itemized out of pytorch's tensor\n",
    "                #print(pred.shape)\n",
    "                #print(y.shape)\n",
    "                pred = pred.argmax(1)\n",
    "                biou = iou(pred,y,2,False)\n",
    "                bfone = fone(pred,y,2,False)\n",
    "                #\n",
    "                batchsizes.append(len(X))\n",
    "                losses.append(loss)\n",
    "                for i in biou:\n",
    "                    ious.extend(i.tolist())\n",
    "                for i in bfone:\n",
    "                    fones.extend(i.tolist())\n",
    "        meanloss = np.average(np.array(losses),weights=np.array(batchsizes))\n",
    "        meaniou = np.ma.mean(np.ma.masked_invalid(ious))\n",
    "        meanfone = np.ma.mean(np.ma.masked_invalid(fones))\n",
    "        print(f\"Validating Error: mIoU: {(100*meaniou):>.02f}%, F1: {(100*meanfone):>.02f}%, Avg loss: {meanloss:>8f}\")\n",
    "        self.history['val_loss'].append(meanloss)\n",
    "        self.history['val_miou'].append(meaniou)\n",
    "        self.history['val_f1'].append(meanfone)\n",
    "        #print(\"Counter: \"+str(earlystop.counter)) #############################\n",
    "        earlystopout = earlystop.early_stop(self.history)\n",
    "        self.stopnow = earlystopout == 1\n",
    "        self.personalbest = earlystopout == -1\n",
    "    def evaluate(self, dataloader):\n",
    "        size = len(dataloader.dataset)\n",
    "        self.model.eval()\n",
    "        num_batches = len(dataloader)\n",
    "        batchsizes, losses, ious, fones = [], [], [], []\n",
    "        with torch.no_grad(): # reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "            for X, y in dataloader:\n",
    "                X = self.rescaler(X).to(device=self.to_device,memory_format=torch.channels_last)\n",
    "                y = y.to(device=self.to_device,dtype=torch.int64)\n",
    "                #ohe_y = ohe(y.to(device=self.to_device,dtype=torch.int64),2)\n",
    "                out = self.model(X)\n",
    "                #\n",
    "                if type(out) is OrderedDict:\n",
    "                    if \"out\" in out:\n",
    "                        loss = self.loss_fn(out[\"out\"], y)\n",
    "                        pred = out[\"out\"]\n",
    "                    else:\n",
    "                        assert(False) #Model doesn't produce output!\n",
    "                    if \"aux\" in out:\n",
    "                        #Check whether aux losses are defined\n",
    "                        if type(self.multi_losses) is OrderedDict and \"aux\" in self.multi_losses:\n",
    "                            #Do aux losses\n",
    "                            loss += self.loss_fn(out[\"aux\"], y)*self.multi_losses[\"aux\"]\n",
    "                        else:\n",
    "                            assert(False) #Unnecessary aux output! Disable it!\n",
    "                else:\n",
    "                    #Good ol' tensor output\n",
    "                    loss = self.loss_fn(out, y)\n",
    "                    pred = out\n",
    "                loss = loss.item() #Itemized out of pytorch's tensor\n",
    "                #print(pred.shape)\n",
    "                #print(y.shape)\n",
    "                pred = pred.argmax(1)\n",
    "                biou = iou(pred,y,2,False)\n",
    "                bfone = fone(pred,y,2,False)\n",
    "                #\n",
    "                batchsizes.append(len(X))\n",
    "                losses.append(loss)\n",
    "                for i in biou:\n",
    "                    ious.extend(i.tolist())\n",
    "                for i in bfone:\n",
    "                    fones.extend(i.tolist())\n",
    "        meanloss = np.average(np.array(losses),weights=np.array(batchsizes))\n",
    "        meaniou = np.ma.mean(np.ma.masked_invalid(ious))\n",
    "        meanfone = np.ma.mean(np.ma.masked_invalid(fones))\n",
    "        return {'loss': meanloss, 'miou': meaniou, 'f1': meanfone}\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X = self.rescaler(X.unsqueeze(0)).to(device=self.to_device,memory_format=torch.channels_last)\n",
    "            out = self.model(X)\n",
    "            #\n",
    "            if type(out) is OrderedDict:\n",
    "                if \"out\" in out:\n",
    "                    pred = out[\"out\"]\n",
    "                else:\n",
    "                    assert(False) #Model doesn't produce output!\n",
    "            else:\n",
    "                #Good ol' tensor output\n",
    "                pred = out\n",
    "            #print(pred.shape)\n",
    "        return pred.squeeze(0)\n",
    "    def train(self, epochs, earlystop, restore_best_weights = True):\n",
    "        while epochs == None or self.epoch < epochs:\n",
    "            if self.scheduler != None:\n",
    "                print(f\"Epoch {self.epoch+1:<6d} ({self.scheduler.get_last_lr()[0]:<5e})------------------------\")\n",
    "            else:\n",
    "                print(f\"Epoch {self.epoch+1:<6d}-------------------------------\")\n",
    "            #with torch.autograd.detect_anomaly():\n",
    "            self.train_one_epoch()\n",
    "            self.validate(earlystop)\n",
    "            if self.personalbest:\n",
    "                if restore_best_weights:\n",
    "                    print('New personal best! Saving best weights...')\n",
    "                    #torch.save(model.state_dict(),'temp-personalbest.pth')\n",
    "                    self.pbrecord = (copy.deepcopy(self.model.state_dict()),copy.deepcopy(self.optimizer.state_dict()))\n",
    "                else:\n",
    "                    print('New personal best!')\n",
    "                self.history['best_epoch'] = self.epoch #zero-indexed\n",
    "            if self.scheduler != None:\n",
    "                if type(self.scheduler) is torch.optim.lr_scheduler.ReduceLROnPlateau:\n",
    "                    lastlr = self.scheduler.get_last_lr()[0]\n",
    "                    self.scheduler.step(self.history['val_loss'][self.epoch])\n",
    "                    if lastlr != self.scheduler.get_last_lr()[0]:\n",
    "                        self.restorebestweights(True)\n",
    "                        earlystop.counter = 0\n",
    "                    #print(\"Cooldown: \"+str(self.scheduler.cooldown_counter))\n",
    "                else:\n",
    "                    self.scheduler.step()\n",
    "            print()\n",
    "            if self.stopnow:\n",
    "                print('Stop.')\n",
    "                print()\n",
    "                break\n",
    "            print()\n",
    "            self.epoch += 1\n",
    "        print('Training ended.')\n",
    "        if restore_best_weights:\n",
    "            self.restorebestweights(False)\n",
    "        return self.history\n",
    "    def restorebestweights(self, revert = False):\n",
    "        if revert:\n",
    "            print('Reverting to best epoch...')\n",
    "            self.model.load_state_dict(self.pbrecord[0])\n",
    "            #self.optimizer.load_state_dict(self.pbrecord[1])\n",
    "            del self.history['train_loss'][self.history['best_epoch']+1:]\n",
    "            del self.history['train_miou'][self.history['best_epoch']+1:]\n",
    "            del self.history['train_f1'][self.history['best_epoch']+1:]\n",
    "            del self.history['val_loss'][self.history['best_epoch']+1:]\n",
    "            del self.history['val_miou'][self.history['best_epoch']+1:]\n",
    "            del self.history['val_f1'][self.history['best_epoch']+1:]\n",
    "            self.epoch = self.history['best_epoch']\n",
    "        else:\n",
    "            print('Restoring best weights...')\n",
    "            self.model.load_state_dict(self.pbrecord[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b49834-2b16-4d78-9ead-b9eacf9ad45c",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a7137-3869-45f1-a465-ed610a2442b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = r'C:\\Users\\User\\Documents\\AAAAA\\eseg2\\Fish EUS.v3i.coco-segmentation-fix'\n",
    "DATASETNAME = 'FishV1'\n",
    "\n",
    "train_img_dir = dataset_dir+r'\\train'\n",
    "val_img_dir = dataset_dir+r'\\valid'\n",
    "test_img_dir = dataset_dir+r'\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e9dbb-8302-49c9-87be-079f7bb35acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lazies(model):\n",
    "    # Initialize lazy modules by passing at least something\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        out = model(torch.rand(1, 3, HEIGHT, WIDTH).to(device=DEVICE,dtype=DTYPE))\n",
    "        print(out.shape)\n",
    "def init_weights(m):\n",
    "    print(type(m))\n",
    "    if type(m) is torch.nn.Linear or type(m) is torch.nn.LazyLinear\\\n",
    "    or type(m) is torch.nn.Conv2d or type(m) is torch.nn.LazyConv2d\\\n",
    "    or type(m) is torch.nn.ConvTranspose2d or type(m) is torch.nn.LazyConvTranspose2d:\n",
    "        if type(m.weight) is torch.nn.UninitializedParameter:\n",
    "            print(\"Skipping uninitialized weights....\")\n",
    "            assert(False)\n",
    "        print(\"Shape: \"+str(m.weight.shape)+\" + \"+str(m.bias.shape if m.bias is not None else \"\"))\n",
    "        if not m.weight.requires_grad:\n",
    "            print(\"Weights frozen, skipping....\")\n",
    "        elif len(m.weight.shape) > 1:\n",
    "            #torch.nn.init.xavier_normal_(m.weight, gain=1.4142135623730950488016887242097)\n",
    "            torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            #torch.nn.init.normal_(m.weight,0,0.01)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "            print(\"Applied He initializer!\")\n",
    "        else:\n",
    "            print(\"He initializer not applicable for 1 dimensional tensor.\")\n",
    "def init_eye(m,x=1):\n",
    "    w = torch.zeros(m.weight.shape, dtype=m.weight.dtype)\n",
    "    w[list(range(m.weight.shape[0])),list(range(m.weight.shape[1]))]=x\n",
    "    m.weight.data.copy_(w)\n",
    "    if m.bias is not None:\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "    print(\"Applied... I... initializer!\")\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "    def __call__(self, tensor, tensor2):\n",
    "        return tensor + torch.randn(tensor.size(),device=tensor.device) * self.std + self.mean, tensor2\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "class Mosaic(object):\n",
    "    def __init__(self, minfrac=0.25, maxfrac=0.75):\n",
    "        self.minfrac = minfrac\n",
    "        self.maxfrac = maxfrac\n",
    "    def __call__(self, images, labels):\n",
    "        if images.shape[0] < 4:\n",
    "            assert(False)\n",
    "        x, y = images.shape[2], images.shape[3]\n",
    "        imagetile = torch.zeros(images.shape,dtype=images.dtype,device=images.device)\n",
    "        labeltile = torch.zeros(labels.shape,dtype=images.dtype,device=images.device)\n",
    "        for i in range(images.shape[0]):\n",
    "            xc, yc = torch.randint(round(x * self.minfrac), round(x * self.maxfrac), ()).item(), torch.randint(round(y * self.minfrac), round(y * self.maxfrac), ()).item()\n",
    "            indices = torch.randperm(images.shape[0]).tolist()\n",
    "            for j in range(4):\n",
    "                match j:\n",
    "                    case 0: #Top left\n",
    "                        x1,y1,x2,y2 = 0,0,xc,yc\n",
    "                        u1,v1,u2,v2 = x-xc,y-yc,x,y\n",
    "                    case 1: #Top right\n",
    "                        x1,y1,x2,y2 = xc,0,x,yc\n",
    "                        u1,v1,u2,v2 = 0,y-yc,x-xc,y\n",
    "                    case 2: #Bottom left\n",
    "                        x1,y1,x2,y2 = 0,yc,xc,y\n",
    "                        u1,v1,u2,v2 = x-xc,0,x,y-yc\n",
    "                    case 3: #Bottom right\n",
    "                        x1,y1,x2,y2 = xc,yc,x,y\n",
    "                        u1,v1,u2,v2 = 0,0,x-xc,y-yc\n",
    "                imagetile[i][0:images.shape[1],y1:y2,x1:x2] = images[indices[j]][0:images.shape[1],v1:v2,u1:u2]\n",
    "                labeltile[i][y1:y2,x1:x2] = labels[indices[j]][v1:v2,u1:u2]\n",
    "        return imagetile, labeltile\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(minfrac={0}, maxfrac={1})'.format(self.minfrac, self.maxfrac)\n",
    "class CutMixMask(object):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.dist = torch.distributions.Beta(torch.tensor([self.alpha]), torch.tensor([self.alpha]))\n",
    "    def __call__(self, images, labels):\n",
    "        if images.shape[0] < 2:\n",
    "            assert(False)\n",
    "        x, y = images.shape[2], images.shape[3]\n",
    "        imagetile = torch.zeros(images.shape,dtype=images.dtype,device=images.device)\n",
    "        labeltile = torch.zeros(labels.shape,dtype=images.dtype,device=images.device)\n",
    "        for i in range(images.shape[0]):\n",
    "            #Generate\n",
    "            lam = float(self.dist.sample(()))\n",
    "            xc, yc = torch.randint(0, x, ()).item(), torch.randint(0, y, ()).item()\n",
    "            r = 0.5 * math.sqrt(1.0 - lam)\n",
    "            xs, ys = round(r * x), round(r * y)\n",
    "            x1, y1 = max(0,xc-xs), max(0,yc-ys)\n",
    "            x2, y2 = min(x,xc+xs), min(y,yc+ys)\n",
    "            #adjusted lam not needed, label will be a mask\n",
    "            index = torch.randint(0, images.shape[0]-1, ()).item()\n",
    "            index += 1 if index >= i else 0\n",
    "            #Cut and Mix!\n",
    "            imagetile[i][0:images.shape[1]] = images[i][0:images.shape[1]]\n",
    "            imagetile[i][0:images.shape[1],y1:y2,x1:x2] = images[index][0:images.shape[1],y1:y2,x1:x2]\n",
    "            labeltile[i] = labels[i]\n",
    "            labeltile[i][y1:y2,x1:x2] = labels[index][y1:y2,x1:x2]\n",
    "        return imagetile, labeltile\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(alpha={0})'.format(self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd66b39b-3375-4236-9a2a-3f186dfeb560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DTYPE = torch.float32\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "EPOCHS = 1200\n",
    "CLASSES = 2\n",
    "AUXLOSSES = None\n",
    "UNPACKER = None\n",
    "scheduler = None\n",
    "\n",
    "SAVEMODEL = False\n",
    "\n",
    "#HEIGHT, WIDTH = 224, 224\n",
    "HEIGHT, WIDTH = 448, 448\n",
    "\n",
    "VGG16_Path = r'C:\\Users\\User\\Documents\\AAAAA\\eseg2\\VGG16-448^2-Fishv2_CLS_DE_AA+CutMixUp-SGD_1e-4_0,9-CE_wprop-BS8-E87-68,08%_34,04%_45,45%.pth'\n",
    "PRE_VGG16_Path = r'C:\\Users\\User\\Documents\\AAAAA\\eseg2\\PRE_VGG16-448^2-Fishv2_CLS_DE_AA+CutMixUp-SGD_1e-4_0,9-CE_wprop-BS8-E73-88,26%_61,70%_61,36%.pth'\n",
    "ResNet50_Path = r'C:\\Users\\User\\Documents\\AAAAA\\eseg2\\ResNet50-448^2-Fishv2_CLS_DE_AA+CutMixUp-SGD_1e-3_0,9_1e-4-CE_wprop-BS8-E40-39,91%_34,04%_40,91%.pth'\n",
    "PRE_ResNet50_Path = r'C:\\Users\\User\\Documents\\AAAAA\\eseg2\\PRE_ResNet50-448^2-Fishv2_CLS_DE_AA+CutMixUp-SGD_1e-3_0,9_1e-4-CE_wprop-BS8-E50-98,59%_68,09%_63,64%.pth'\n",
    "\n",
    "'''Model selection:\n",
    "UNet\n",
    "UNet+VGG16\n",
    "UNet+Pretrained_VGG16\n",
    "UNet+Pre_CLS_VGG16\n",
    "UNet+CLS_VGG16\n",
    "UNet+ResNet50\n",
    "UNet+Pretrained_ResNet50\n",
    "UNet+Pre_CLS_ResNet50\n",
    "UNet+CLS_ResNet50\n",
    "\n",
    "FCN8+VGG16\n",
    "FCN8+Pretrained_VGG16\n",
    "FCN8+Pre_CLS_VGG16\n",
    "FCN8+Pre_CLS_VGG16+4096\n",
    "FCN8+CLS_VGG16\n",
    "FCN8+ResNet50\n",
    "FCN8+Pretrained_ResNet50\n",
    "FCN8+Pre_CLS_ResNet50\n",
    "FCN8+CLS_ResNet50\n",
    "\n",
    "SegNet16\n",
    "Pretrained_SegNet16\n",
    "Pretrained_SegNet16+PTDec\n",
    "Pre_CLS_SegNet16\n",
    "CLS_SegNet16\n",
    "\n",
    "UNet+Alt\n",
    "UNet+Pretrained_ResNet50+Alt\n",
    "'''\n",
    "\n",
    "MODELNAME = 'UNet+Pre_CLS_VGG16'\n",
    "\n",
    "'''Optimizer selection:\n",
    "SGD_1e_4_0_99 #UNet -> Good\n",
    "SGD_3e_4_0_99 #\n",
    "SGD_3e_4_0_99_3e_5 #\n",
    "SGD_1e_3_0_99 #FCN+VGG -> Bad/Ok? #FCN+ResNet -> Ok\n",
    "SGD_1e_3_0_99_5e_4 #FCN+VGG -> Good #FCN+ResNet -> Ok\n",
    "SGD_1e_3_0_99_2e_4 #FCN+VGG -> ??? #FCN+ResNet -> Ok\n",
    "SGDPoly_1e_2_0_99\n",
    "SGD_1e_1_0_9 #SegNet -> Good\n",
    "SGD_3e_2_0_9\n",
    "SGD_1e_2_0_9 #Slower training\n",
    "SGDDiff_1e_4_0_99\n",
    "SGDExp_1e_2_0_99\n",
    "Adam_1e_4\n",
    "Adam_3e_5\n",
    "'''\n",
    "\n",
    "OPTIMIZERNAME = 'SGD_1e_4_0_99'\n",
    "\n",
    "ARGS = ARGNAME.split(\":\")\n",
    "print(ARGS)\n",
    "if not ARGNAME.startswith('C:\\\\'):\n",
    "    if ARGS[0] == '--Model':\n",
    "        MODELNAME = ARGS[1]\n",
    "    if ARGS[2] == '--Optimizer':\n",
    "        OPTIMIZERNAME = ARGS[3]\n",
    "\n",
    "match MODELNAME:\n",
    "    #########\n",
    "    # U-Net #\n",
    "    #########\n",
    "    case 'UNet':\n",
    "        model = UNet(3,CLASSES).to(device=DEVICE,dtype=DTYPE)\n",
    "        model.apply(init_weights)\n",
    "    case 'UNet+VGG16':\n",
    "        backbone = torch.hub.load('pytorch/vision', 'vgg16_bn', weights=None)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        convblocks = torch.nn.ModuleList(\n",
    "            [backbone.features[0:6],backbone.features[6:13],backbone.features[13:23],backbone.features[23:33],backbone.features[33:43]]\n",
    "        )\n",
    "        model = UNet(3,CLASSES,convblocks).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        del backbone\n",
    "    case 'UNet+Pretrained_VGG16':\n",
    "        backbone = torch.hub.load('pytorch/vision', 'vgg16_bn', weights='VGG16_BN_Weights.IMAGENET1K_V1')\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList(\n",
    "            [backbone.features[0:6],backbone.features[6:13],backbone.features[13:23],backbone.features[23:33],backbone.features[33:43]]\n",
    "        )\n",
    "        model = UNet(3,CLASSES,convblocks).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case 'UNet+Pre_CLS_VGG16':\n",
    "        backbone = torch.load(PRE_VGG16_Path)\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList(\n",
    "            [backbone.features[0:6],backbone.features[6:13],backbone.features[13:23],backbone.features[23:33],backbone.features[33:43]]\n",
    "        )\n",
    "        model = UNet(3,CLASSES,convblocks,channels=[64,128,256,512,512]).to(device=DEVICE,dtype=DTYPE) #################################################################################\n",
    "        init_lazies(model) #,channels=[64,128,256,512,512]\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case 'UNet+CLS_VGG16':\n",
    "        backbone = torch.load(VGG16_Path)\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList(\n",
    "            [backbone.features[0:6],backbone.features[6:13],backbone.features[13:23],backbone.features[23:33],backbone.features[33:43]]\n",
    "        )\n",
    "        model = UNet(3,CLASSES,convblocks).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case 'UNet+ResNet50':\n",
    "        backbone = torch.hub.load('pytorch/vision', 'resnet50', weights=None)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        convblocks = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(backbone.conv1,backbone.bn1,backbone.relu),\n",
    "            torch.nn.Sequential(backbone.maxpool,backbone.layer1),\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "            backbone.layer4\n",
    "        ])\n",
    "        model = UNet(3,CLASSES,convblocks,upsample_output=2).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        del backbone\n",
    "    case 'UNet+Pretrained_ResNet50':\n",
    "        backbone = torch.hub.load('pytorch/vision', 'resnet50', weights='ResNet50_Weights.IMAGENET1K_V2')\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(backbone.conv1,backbone.bn1,backbone.relu),\n",
    "            torch.nn.Sequential(backbone.maxpool,backbone.layer1),\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "            backbone.layer4\n",
    "        ])\n",
    "        model = UNet(3,CLASSES,convblocks,upsample_output=2).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case 'UNet+Pre_CLS_ResNet50':\n",
    "        backbone = torch.load(PRE_ResNet50_Path)\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(backbone.conv1,backbone.bn1,backbone.relu),\n",
    "            torch.nn.Sequential(backbone.maxpool,backbone.layer1),\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "            backbone.layer4\n",
    "        ])\n",
    "        model = UNet(3,CLASSES,convblocks,upsample_output=2).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case 'UNet+CLS_ResNet50':\n",
    "        backbone = torch.load(ResNet50_Path)\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(backbone.conv1,backbone.bn1,backbone.relu),\n",
    "            torch.nn.Sequential(backbone.maxpool,backbone.layer1),\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "            backbone.layer4\n",
    "        ])\n",
    "        model = UNet(3,CLASSES,convblocks,upsample_output=2).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    #######\n",
    "    # FCN #\n",
    "    #######\n",
    "    case 'FCN8+VGG16':\n",
    "        backbone = torch.hub.load('pytorch/vision', 'vgg16_bn', weights=None)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        convblocks = torch.nn.ModuleList(\n",
    "            [backbone.features[0:7],backbone.features[7:14],backbone.features[14:24],backbone.features[24:34],backbone.features[34:44]]\n",
    "        )\n",
    "        model = FCNBase(3,CLASSES,convblocks,upsamplevar=8).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        #for m in [model.classifier3,model.classifier4]:\n",
    "        #    torch.nn.init.zeros_(m.weight) #zero-init scoring layers with a skip\n",
    "        #    torch.nn.init.zeros_(m.bias)\n",
    "        del backbone\n",
    "    case 'FCN8+Pretrained_VGG16':\n",
    "        backbone = torch.hub.load('pytorch/vision', 'vgg16_bn', weights='VGG16_BN_Weights.IMAGENET1K_V1')\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList(\n",
    "            [backbone.features[0:7],backbone.features[7:14],backbone.features[14:24],backbone.features[24:34],backbone.features[34:44]]\n",
    "        )\n",
    "        model = FCNBase(3,CLASSES,convblocks,upsamplevar=8).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case 'FCN8+Pre_CLS_VGG16':\n",
    "        backbone = torch.load(PRE_VGG16_Path)\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList(\n",
    "            [backbone.features[0:7],backbone.features[7:14],backbone.features[14:24],backbone.features[24:34],backbone.features[34:44]]\n",
    "        )\n",
    "        model = FCNBase(3,CLASSES,convblocks,upsamplevar=8).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case 'FCN8+Pre_CLS_VGG16+4096':\n",
    "        backbone = torch.load(PRE_VGG16_Path)\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList(\n",
    "            [backbone.features[0:7],backbone.features[7:14],backbone.features[14:24],backbone.features[24:34],backbone.features[34:44]]\n",
    "        )\n",
    "        convblocks[4].append(torch.nn.Conv2d(512,4096,kernel_size=(3,3),padding=(1,1),bias=False))\n",
    "        convblocks[4].append(torch.nn.BatchNorm2d(4096))\n",
    "        convblocks[4].append(torch.nn.ReLU(inplace=True))\n",
    "        convblocks[4].append(torch.nn.Conv2d(4096,4096,kernel_size=(3,3),padding=(1,1),bias=False))\n",
    "        convblocks[4].append(torch.nn.BatchNorm2d(4096))\n",
    "        convblocks[4].append(torch.nn.ReLU(inplace=True))\n",
    "        convblocks[4].append(torch.nn.Conv2d(4096,CLASSES,kernel_size=(1,1),bias=False))\n",
    "        model = FCNBase(3,CLASSES,convblocks,upsamplevar=8).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case 'FCN8+CLS_VGG16':\n",
    "        backbone = torch.load(VGG16_Path)\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList(\n",
    "            [backbone.features[0:7],backbone.features[7:14],backbone.features[14:24],backbone.features[24:34],backbone.features[34:44]]\n",
    "        )\n",
    "        model = FCNBase(3,CLASSES,convblocks,upsamplevar=8).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case 'FCN8+ResNet50':\n",
    "        backbone = torch.hub.load('pytorch/vision', 'resnet50', weights=None)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        convblocks = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(backbone.conv1,backbone.bn1,backbone.relu),\n",
    "            torch.nn.Sequential(backbone.maxpool,backbone.layer1),\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "            backbone.layer4\n",
    "        ])\n",
    "        model = FCNBase(3,CLASSES,convblocks,upsamplevar=8).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        del backbone\n",
    "    case 'FCN8+Pretrained_ResNet50':\n",
    "        backbone = torch.hub.load('pytorch/vision', 'resnet50', weights='ResNet50_Weights.IMAGENET1K_V2')\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(backbone.conv1,backbone.bn1,backbone.relu),\n",
    "            torch.nn.Sequential(backbone.maxpool,backbone.layer1),\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "            backbone.layer4\n",
    "        ])\n",
    "        model = FCNBase(3,CLASSES,convblocks,upsamplevar=8).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case 'FCN8+Pre_CLS_ResNet50':\n",
    "        backbone = torch.load(PRE_ResNet50_Path)\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(backbone.conv1,backbone.bn1,backbone.relu),\n",
    "            torch.nn.Sequential(backbone.maxpool,backbone.layer1),\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "            backbone.layer4\n",
    "        ])\n",
    "        model = FCNBase(3,CLASSES,convblocks,upsamplevar=8).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case 'FCN8+CLS_ResNet50':\n",
    "        backbone = torch.load(ResNet50_Path)\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(backbone.conv1,backbone.bn1,backbone.relu),\n",
    "            torch.nn.Sequential(backbone.maxpool,backbone.layer1),\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "            backbone.layer4\n",
    "        ])\n",
    "        model = FCNBase(3,CLASSES,convblocks,upsamplevar=8).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    ##########\n",
    "    # SegNet #\n",
    "    ##########\n",
    "    case 'SegNet16':\n",
    "        backbone = torch.hub.load('pytorch/vision', 'vgg16_bn', weights=None)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        convblocks = torch.nn.ModuleList(\n",
    "            [backbone.features[0:6],backbone.features[7:13],backbone.features[14:23],backbone.features[24:33],backbone.features[34:43],]\n",
    "        )\n",
    "        model = SegNet(3,CLASSES,convblocks=convblocks).to(device=DEVICE,dtype=DTYPE)\n",
    "        model.apply(init_weights)\n",
    "        del backbone\n",
    "    case 'Pretrained_SegNet16':\n",
    "        backbone = torch.hub.load('pytorch/vision', 'vgg16_bn', weights='VGG16_BN_Weights.IMAGENET1K_V1')\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList(\n",
    "            [backbone.features[0:6],backbone.features[7:13],backbone.features[14:23],backbone.features[24:33],backbone.features[34:43],]\n",
    "        )\n",
    "        model = SegNet(3,CLASSES,convblocks=convblocks).to(device=DEVICE,dtype=DTYPE)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case 'Pretrained_SegNet16+PTDec':\n",
    "        backbone = torch.hub.load('pytorch/vision', 'vgg16_bn', weights='VGG16_BN_Weights.IMAGENET1K_V1')\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList(\n",
    "            [backbone.features[0:6],backbone.features[7:13],backbone.features[14:23],backbone.features[24:33],backbone.features[34:43],]\n",
    "        )\n",
    "        model = SegNet(3,CLASSES,convblocks=convblocks,ptdec=True).to(device=DEVICE,dtype=DTYPE)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case 'Pre_CLS_SegNet16':\n",
    "        backbone = torch.load(PRE_VGG16_Path)\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList(\n",
    "            [backbone.features[0:6],backbone.features[7:13],backbone.features[14:23],backbone.features[24:33],backbone.features[34:43],]\n",
    "        )\n",
    "        model = SegNet(3,CLASSES,convblocks=convblocks).to(device=DEVICE,dtype=DTYPE)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case 'CLS_SegNet16':\n",
    "        backbone = torch.load(VGG16_Path)\n",
    "        backbone.requires_grad_(False) #Freeze the pretrained weights before applying init\n",
    "        convblocks = torch.nn.ModuleList(\n",
    "            [backbone.features[0:6],backbone.features[7:13],backbone.features[14:23],backbone.features[24:33],backbone.features[34:43],]\n",
    "        )\n",
    "        model = SegNet(3,CLASSES,convblocks=convblocks).to(device=DEVICE,dtype=DTYPE)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    #######\n",
    "    # ??? #\n",
    "    #######\n",
    "    case 'UNet+Alt':\n",
    "        model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        #model.apply(init_weights)\n",
    "        #del backbone\n",
    "    case 'UNet+Pretrained_ResNet50+Alt':\n",
    "        backbone = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', weights='FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1').backbone\n",
    "        #backbone.conv1.stride = (1,1)\n",
    "        convblocks = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(backbone.conv1,backbone.bn1,backbone.relu),\n",
    "            torch.nn.Sequential(backbone.maxpool,backbone.layer1),\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "            backbone.layer4\n",
    "        ])\n",
    "        model = UNet(3,CLASSES,convblocks,upsample_output=2).to(device=DEVICE,dtype=DTYPE)\n",
    "        init_lazies(model)\n",
    "        model.apply(init_weights)\n",
    "        backbone.requires_grad_(True) #Unfreeze\n",
    "        del backbone\n",
    "    case _:\n",
    "        assert(False)\n",
    "\n",
    "match OPTIMIZERNAME:\n",
    "    case 'SGD_1e_4_0_99':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.99, weight_decay=0) #Default\n",
    "    case 'SGD_3e_4_0_99':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=3e-4, momentum=0.99, weight_decay=0) #\n",
    "    case 'SGD_3e_4_0_99_3e_5':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=3e-4, momentum=0.99, weight_decay=3e-5) #\n",
    "    case 'SGD_1e_3_0_99':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.99, weight_decay=0) #\n",
    "    case 'SGD_1e_3_0_99_5e_4':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.99, weight_decay=5e-4) #\n",
    "    case 'SGD_1e_3_0_99_2e_4':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.99, weight_decay=2e-4) #\n",
    "    case 'SGDPoly_1e_2_0_99':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.99, weight_decay=0) #Default\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.316227766, patience=25, threshold=0, cooldown=0, min_lr=1e-4)\n",
    "        scheduler.cooldown_counter = 25\n",
    "    case 'SGD_1e_1_0_9':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9, weight_decay=0) #Default\n",
    "    case 'SGD_3e_2_0_9':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=3e-2, momentum=0.9, weight_decay=0) #\n",
    "    case 'SGD_1e_2_0_9':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=0) #\n",
    "    case 'SGDDiff_1e_4_0_99':\n",
    "        encoder_params = set(model.encoderblocks.parameters())\n",
    "        not_encoder_params = list(set(model.parameters()) - encoder_params)\n",
    "        encoder_params = list(encoder_params)\n",
    "        optimizer = torch.optim.SGD(\n",
    "            [\n",
    "                {\"params\": not_encoder_params},\n",
    "                {\"params\": encoder_params, \"lr\": 1e-5}\n",
    "            ],\n",
    "            lr=1e-4, momentum=0.99, weight_decay=0\n",
    "        )\n",
    "    case 'SGDExp_1e_2_0_99':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.99, weight_decay=0) #Default\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,  gamma=0.977237221, verbose=True) #0.1^(1/100)\n",
    "    case 'Adam_1e_4':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "    case 'Adam_3e_5':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n",
    "    case _:\n",
    "        assert(False)\n",
    "\n",
    "antilatestopper = EarlyStopper('val_loss',lower_is_better=True,patience=50,grace_period=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71dc3b2-fcb7-4977-9cfe-0348581b7174",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(MODELNAME)\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ad07a-e001-4d74-baee-132036bb51a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = ImageGenerator(test_img_dir,size=(HEIGHT,WIDTH))\n",
    "image, mask = ig.__getitem__(4)\n",
    "print()\n",
    "print(torch.nn.functional.one_hot(mask.long(),2).dtype)\n",
    "print(mask.shape)\n",
    "display([{'image':image,'mask':mask}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9d344-3bcf-4d4c-87d2-682b3a0f6bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds = ImageGenerator(train_img_dir,size=(HEIGHT,WIDTH))\n",
    "val_ds = ImageGenerator(val_img_dir,size=(HEIGHT,WIDTH))\n",
    "test_ds = ImageGenerator(test_img_dir,size=(HEIGHT,WIDTH))\n",
    "\n",
    "PREPREPROCESSOR = 'MosaicMix' #MosaicMix is better than None and PureMosaic\n",
    "\n",
    "match PREPREPROCESSOR:\n",
    "    case 'None':\n",
    "        pretransform = torchvision.transforms.v2.ToDtype(torch.float32, True)\n",
    "    case 'MosaicMix':\n",
    "        pretransform = torchvision.transforms.v2.Compose([\n",
    "            torchvision.transforms.v2.ToDtype(torch.float32, True),\n",
    "            torchvision.transforms.v2.RandomChoice([\n",
    "                torchvision.transforms.v2.RandomAffine(0),\n",
    "                Mosaic()\n",
    "            ])\n",
    "        ])\n",
    "    case 'PureMosaic':\n",
    "        pretransform = torchvision.transforms.v2.Compose([\n",
    "            torchvision.transforms.v2.ToDtype(torch.float32, True),\n",
    "            Mosaic()\n",
    "        ])\n",
    "    case 'CutMix':\n",
    "        pretransform = torchvision.transforms.v2.Compose([\n",
    "            torchvision.transforms.v2.ToDtype(torch.float32, True),\n",
    "            torchvision.transforms.v2.RandomChoice([\n",
    "                torchvision.transforms.v2.RandomAffine(0),\n",
    "                CutMixMask()\n",
    "            ])\n",
    "        ])\n",
    "    case 'PureCut':\n",
    "        pretransform = torchvision.transforms.v2.Compose([\n",
    "            torchvision.transforms.v2.ToDtype(torch.float32, True),\n",
    "            CutMixMask()\n",
    "        ])\n",
    "    case 'MosaicCutMix':\n",
    "        pretransform = torchvision.transforms.v2.Compose([\n",
    "            torchvision.transforms.v2.ToDtype(torch.float32, True),\n",
    "            torchvision.transforms.v2.RandomChoice([\n",
    "                Mosaic(),\n",
    "                CutMixMask()\n",
    "            ])\n",
    "        ])\n",
    "    case _:\n",
    "        assert(False)\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    return pretransform(*torch.utils.data.default_collate(batch))\n",
    "\n",
    "if DATASETNAME == \"FishV3\":\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=True,pin_memory=True)\n",
    "else:\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=True,pin_memory=True,collate_fn=collate_fn,drop_last=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_ds,batch_size=BATCH_SIZE,pin_memory=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_ds,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43408ce-a209-449d-af18-2e5bb524ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.v2.Compose([\n",
    "    AddGaussianNoise(0,0.0125),\n",
    "    torchvision.transforms.v2.RandomChoice([\n",
    "        torchvision.transforms.v2.RandomAffine(0),\n",
    "        torchvision.transforms.v2.GaussianBlur(5),\n",
    "        torchvision.transforms.v2.GaussianBlur(9),\n",
    "    ]),\n",
    "    torchvision.transforms.v2.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.v2.RandomVerticalFlip(p=0.5),\n",
    "    torchvision.transforms.v2.ColorJitter(0.25,0.0,0.25),\n",
    "    torchvision.transforms.v2.RandomChoice([\n",
    "        torchvision.transforms.v2.RandomAffine(0),\n",
    "        torchvision.transforms.v2.ElasticTransform(alpha=768, sigma=14),\n",
    "        torchvision.transforms.v2.ElasticTransform(alpha=2048, sigma=20)\n",
    "    ]),\n",
    "    torchvision.transforms.v2.RandomRotation((-15,15))\n",
    "]).to(DEVICE)\n",
    "\n",
    "loss_fn = FocalTversky_loss({'apply_nonlin':torch.nn.Softmax(dim=1)}) #gamma from 0.333... to 1, default is 0.75\n",
    "LOSSNAME = 'FocalTversky'\n",
    "\n",
    "\n",
    "model_manager = Manager(train_dataloader, val_dataloader, model, loss_fn, optimizer=optimizer, preprocesser=transforms,scheduler=scheduler,multi_losses=AUXLOSSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1edc4d-1762-4734-81f6-761c515de804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_history = None\n",
    "antilatestopper.reset()\n",
    "model_history = model_manager.train(EPOCHS, antilatestopper, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82beb3e-8299-4ae6-8453-a70168790f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_history == None:\n",
    "    print(\"Missing history, restoring...\")\n",
    "    model_history = model_manager.history "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089662f4-cad1-4c73-ad89-bcf111bdc71d",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00fa748-73cf-42b6-8be2-2a56546f9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_ds,batch_size=BATCH_SIZE)\n",
    "del val_dataloader\n",
    "val_dataloader = torch.utils.data.DataLoader(val_ds,batch_size=BATCH_SIZE)\n",
    "train_metric = model_manager.evaluate(train_dataloader)\n",
    "val_metric = model_manager.evaluate(val_dataloader)\n",
    "test_metric = model_manager.evaluate(test_dataloader)\n",
    "\n",
    "print(f\"Training Error: mIoU: {(100*train_metric['miou']):>.3f}%, F1: {(100*train_metric['f1']):>.3f}%, Avg loss: {train_metric['loss']:>8f}\")\n",
    "print(f\"Validating Error: mIoU: {(100*val_metric['miou']):>.3f}%, F1: {(100*val_metric['f1']):>.3f}%, Avg loss: {val_metric['loss']:>8f}\")\n",
    "print(f\"Testing Error: mIoU: {(100*test_metric['miou']):>.3f}%, F1: {(100*test_metric['f1']):>.3f}%, Avg loss: {test_metric['loss']:>8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643af2cf-cb84-499c-97b0-f5665a4f66da",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metrics = ('{:.2%}'.format(train_metric['miou'])+'_'+'{:.2%}'.format(val_metric['miou'])+'_'+'{:.2%}'.format(test_metric['miou'])).replace('.',',')\n",
    "size = str(WIDTH)+'^2' if WIDTH == HEIGHT else str(WIDTH)+''+str(HEIGHT)\n",
    "precision_name = \"-half\" if next(model.parameters()).dtype == torch.float16 else \"-bhalf\" if next(model.parameters()).dtype == torch.bfloat16 else ''\n",
    "NAME = MODELNAME+precision_name+'-'+size+'-'+DATASETNAME+\"+\"+PREPREPROCESSOR+'-'+OPTIMIZERNAME+'-'+LOSSNAME+'-BS'+str(BATCH_SIZE) \\\n",
    "+'-E'+str(model_history['best_epoch'])+'-'+best_metrics\n",
    "print(NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679d3ba-ed34-40fd-a9f7-a65f3e60edb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ig = ImageGenerator(train_img_dir,size=(HEIGHT,WIDTH))\n",
    "display_list = []\n",
    "for i in range(len(ig)):\n",
    "    image, mask = ig.__getitem__(i)\n",
    "    prediction = model_manager.predict(image).detach().argmax(0).cpu()\n",
    "    display_list.append({'image':image, 'mask':mask, 'pred':prediction})\n",
    "display(display_list,NAME+'-TrainResult')\n",
    "\n",
    "ig = ImageGenerator(val_img_dir,size=(HEIGHT,WIDTH))\n",
    "display_list = []\n",
    "for i in range(len(ig)):\n",
    "    image, mask = ig.__getitem__(i)\n",
    "    prediction = model_manager.predict(image).detach().argmax(0).cpu()\n",
    "    display_list.append({'image':image, 'mask':mask, 'pred':prediction})\n",
    "display(display_list,NAME+'-ValResult')\n",
    "\n",
    "ig = ImageGenerator(test_img_dir,size=(HEIGHT,WIDTH))\n",
    "display_list = []\n",
    "for i in range(len(ig)):\n",
    "    image, mask = ig.__getitem__(i)\n",
    "    prediction = model_manager.predict(image).detach().argmax(0).cpu()\n",
    "    display_list.append({'image':image, 'mask':mask, 'pred':prediction})\n",
    "display(display_list,NAME+'-TestResult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774727cc-c58e-4bea-af17-80a73672b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = model_history['train_loss']\n",
    "val_loss = model_history['val_loss']\n",
    "train_miou = model_history['train_miou']\n",
    "val_miou = model_history['val_miou']\n",
    "train_f1 = model_history['train_f1']\n",
    "val_f1 = model_history['val_f1']\n",
    "best_epoch = model_history['best_epoch']\n",
    "plt.figure(figsize=(21, 7))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(len(train_loss)), train_loss, '-r', label='Training')\n",
    "plt.plot(range(len(val_loss)), val_loss, '-b', label='Validation')\n",
    "plt.axvline(x=best_epoch)\n",
    "plt.title(LOSSNAME+' Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "if type(loss_fn) is DC_and_CE_loss:\n",
    "    plt.ylim([-1, 1])\n",
    "elif type(loss_fn) is SoftDiceLoss or (type(loss_fn) is TverskyLoss and type(loss_fn) is not FocalTversky_loss):\n",
    "    plt.ylim([-1, 0])\n",
    "else:\n",
    "    plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(len(train_miou)), train_miou, '-r', label='Training')\n",
    "plt.plot(range(len(val_miou)), val_miou, '-b', label='Validation')\n",
    "plt.axvline(x=best_epoch)\n",
    "plt.title('Mean Intersection over Union')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('mIoU')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(len(train_f1)), train_f1, '-r', label='Training')\n",
    "plt.plot(range(len(val_f1)), val_f1, '-b', label='Validation')\n",
    "plt.axvline(x=best_epoch)\n",
    "plt.title('Mean F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.savefig(NAME+'-TrainPlot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babfd7e-097c-4157-9cd9-2b463887b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVEMODEL:\n",
    "    torch.save(model, NAME+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60704faf-1f9a-4615-93cb-69b0ce0876f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Log.txt\", \"a\") as f:\n",
    "    f.write('Name: ')\n",
    "    f.write(NAME)\n",
    "    f.write('\\n')\n",
    "    f.write(str(loss_fn))\n",
    "    f.write('\\n')\n",
    "    f.write(str(optimizer))\n",
    "    f.write('\\n')\n",
    "    f.write(str(transforms))\n",
    "    f.write('\\n')\n",
    "    f.write('-------------------------------\\n')\n",
    "    f.write('\\n')\n",
    "with open(\"Table.txt\", \"a\") as f:\n",
    "    f.write(MODELNAME+'\\t'+str(WIDTH)+'\\t'+str(HEIGHT)+'\\t'+DATASETNAME+'\\t'+PREPREPROCESSOR+'\\t'+OPTIMIZERNAME+'\\t'+LOSSNAME+'\\t'+str(BATCH_SIZE) \\\n",
    "            +'\\t'+str(model_history['best_epoch'])+'\\t'+str(train_metric['miou'])+'\\t'+str(val_metric['miou'])+'\\t'+str(test_metric['miou']))\n",
    "    f.write('\\n')\n",
    "with open(\"TrainLosses.txt\", \"a\") as f:\n",
    "    for i in train_loss:\n",
    "        f.write(str(i))\n",
    "        f.write('\\t')\n",
    "    f.write('\\n')\n",
    "with open(\"ValLosses.txt\", \"a\") as f:\n",
    "    for i in val_loss:\n",
    "        f.write(str(i))\n",
    "        f.write('\\t')\n",
    "    f.write('\\n')\n",
    "with open(\"TrainMIoU.txt\", \"a\") as f:\n",
    "    for i in train_miou:\n",
    "        f.write(str(i))\n",
    "        f.write('\\t')\n",
    "    f.write('\\n')\n",
    "with open(\"ValMIoU.txt\", \"a\") as f:\n",
    "    for i in val_miou:\n",
    "        f.write(str(i))\n",
    "        f.write('\\t')\n",
    "    f.write('\\n')\n",
    "with open(\"TrainF1.txt\", \"a\") as f:\n",
    "    for i in train_f1:\n",
    "        f.write(str(i))\n",
    "        f.write('\\t')\n",
    "    f.write('\\n')\n",
    "with open(\"ValF1.txt\", \"a\") as f:\n",
    "    for i in val_f1:\n",
    "        f.write(str(i))\n",
    "        f.write('\\t')\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3455e9-8b5c-43ff-b323-417f58765b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = ['-r']*8+[None]*8+['-g']*8+['-b']*8\n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "with open(\"ValLosses.txt\", \"r\", newline='\\r\\n') as f:\n",
    "    index = 0\n",
    "    linestr = f.readline()\n",
    "    while len(linestr) != 0:\n",
    "        line = linestr.split(\"\\t\")\n",
    "        line.remove(\"\\r\\n\")\n",
    "        data = list(map(lambda x: float(x),line))\n",
    "        if index < len(colormap) and colormap[index] != None:\n",
    "            plt.plot(range(len(data)), data, colormap[index], label='XXX')\n",
    "        index += 1\n",
    "        linestr = f.readline()\n",
    "plt.savefig('TrainPlot-seg')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
